{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All loaded data size:8754\n",
      "Will create train, dev, test(s) splits\n",
      "Total params: 58807\n",
      "Starting training:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-967317bc586c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mtrainer_smart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mres_smart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_smart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/repo/latynt/experimenter/experimenter/utils/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mtloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mtloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pair stance classification\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import importlib\n",
    "import numpy as np\n",
    "sys.path.append(\"..\")\n",
    "import copy\n",
    "\n",
    "\n",
    "import experimenter\n",
    "from experimenter import evaluation\n",
    "from experimenter.utils import modeling, training, data, text, utils\n",
    "importlib.reload(experimenter)\n",
    "importlib.reload(text)\n",
    "importlib.reload(modeling)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(training)\n",
    "importlib.reload(data)\n",
    "importlib.reload(utils)\n",
    "\n",
    "configs = {\"epochs\": 25,\n",
    "           \"experiment_output_path\": \"local/final_training.json\", \n",
    "           \"log_interval\": 1, \"model_path\": \"local/model.state\", \n",
    "           \"processor\": {\n",
    "                \"module\": \"experimenter.utils.data\",\n",
    "                \"class\": \"PairStanceProvider\",\n",
    "                \"params\":{\"input_path\": \"/Users/jkhouja/workspace/repo/arabic_media/data/for training/batch_0_to_15000_pairs_eq_weight_random.csv\",\n",
    "                          \"seq_len\": {'inp':[50,50], 'label':[1],'mask':[1]}, \"batch_size\": 4, \"splits\": [.7, .2, .1], \"drop_last\":True,  \"shuffle\": True, \"vocab_path\": \"local/vocab.json\"}},\n",
    "           \"model\":{\n",
    "                \"module\": \"experimenter.utils.modeling\",\n",
    "                \"class\": \"RNNPairModel\",\n",
    "                \"params\":{\"embedding_dim\": 20, \"hidden_dim\": 100, \"num_classes\": 3, \"dropout\": 0, \"max_seq_len\": {\"eval\": 1, \"value\": \"config['processor']['params']['seq_len']['inp'][0]\"}}},\n",
    "           \n",
    "            \"evaluator\":{\n",
    "                \"module\": \"experimenter.evaluation\",\n",
    "                \"class\": \"ListEvaluator\",\n",
    "                \"params\": {\"loss_f\": [{\"module\": 'torch.nn', \"class\": 'CrossEntropyLoss', \"params\": {'reduction': 'none'}}],\n",
    "                          \"metrics_f\": [{\"module\": 'experimenter.evaluation', \"class\": 'Accuracy', \"params\": {}}]}},\n",
    "            \"optimizer\":{\n",
    "                \"module\": \"torch.optim\",\n",
    "                \"class\": \"Adam\",\n",
    "                \"params\":{\n",
    "                    \"params\": {\n",
    "                    \"eval\": 1,\n",
    "                    \"value\": \"config['model']['model'].parameters()\"},\n",
    "                'lr':  0.001}}\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "lm_configs = {\"epochs\": 5,\n",
    "           \"experiment_output_path\": \"local/final_training.json\", \n",
    "           \"log_interval\": 1, \"model_path\": \"local/model.state\", \n",
    "           \"processor\": {\n",
    "                \"module\": \"experimenter.utils.data\",\n",
    "                \"class\": \"LMProvider\",\n",
    "                \"params\":{\"input_path\": \"/Users/jkhouja/workspace/repo/arabic_media/data/pairs/batch_0_to_15000_pairs_eq_weight_random.csv\",\n",
    "                          \"seq_len\": {'inp':[50], 'label':[50],'mask':[1]}, \"batch_size\": 4, \"splits\": [.1, .2, .1, .6], \"drop_last\":\"False\",  \"shuffle\": \"True\", \"vocab_path\": \"local/vocab.json\"}},\n",
    "           \"model\":{\n",
    "                \"module\": \"experimenter.utils.modeling\",\n",
    "                \"class\": \"RNNLMModel\",\n",
    "                \"params\":{\"embedding_dim\": 8, \"hidden_dim\": 30, \"dropout\": 0, \"max_seq_len\": {\"eval\": 1, \"value\": \"config['processor']['params']['seq_len']['inp'][0]\"}}},\n",
    "           \n",
    "            \"evaluator\":{\n",
    "                \"module\": \"experimenter.evaluation\",\n",
    "                \"class\": \"ListEvaluator\",\n",
    "                \"params\": {\"loss_f\": [{\"module\": 'torch.nn', \"class\": 'CrossEntropyLoss', \"params\": {'reduction': 'none'}}]}},\n",
    "            \"optimizer\":{\n",
    "                \"module\": \"torch.optim\",\n",
    "                \"class\": \"Adam\",\n",
    "                \"params\":{\n",
    "                    \"params\": {\n",
    "                    \"eval\": 1,\n",
    "                    \"value\": \"config['model']['model'].parameters()\"},\n",
    "                'lr':  0.001}}\n",
    "           }\n",
    "\n",
    "multi_configs = {\"epochs\": 2,\n",
    "           \"experiment_output_path\": \"local/final_training.json\", \n",
    "           \"log_interval\": 1, \"model_path\": \"local/model.state\", \n",
    "           \"processor\": {\n",
    "                \"module\": \"experimenter.utils.data\",\n",
    "                \"class\": \"MultiLMPairProvider\",\n",
    "                \"params\":{\"input_path\": \"/Users/jkhouja/workspace/repo/arabic_media/data/for training/batch_0_to_15000_pairs_eq_weight_random.csv\",\n",
    "                          \"seq_len\": {'inp':[50, 50], 'label':[50, 1],'mask':[1, 1]}, \"batch_size\": 4, \"splits\": [.7, .2, .1], \"drop_last\":True,  \"shuffle\": True, \"vocab_path\": \"local/vocab.json\"}},\n",
    "           \"model\":{\n",
    "                \"module\": \"experimenter.utils.modeling\",\n",
    "                \"class\": \"RNNMultiLMPairModel\",\n",
    "                \"params\":{\"embedding_dim\": 20, \"hidden_dim\": 100, \"dropout\": 0, \"lm_classes\": {\"eval\": 1, \"value\":\"config['processor']['params']['vocab_size']\"},  \"num_classes\":3 , \"max_seq_len\": {\"eval\": 1, \"value\": \"config['processor']['params']['seq_len']['inp'][0]\"}}},\n",
    "           \n",
    "            \"evaluator\":{\n",
    "                \"module\": \"experimenter.evaluation\",\n",
    "                \"class\": \"ListEvaluator\",\n",
    "                \"params\": {\"loss_f\": [{\"module\": 'torch.nn', \"class\": 'CrossEntropyLoss', \"params\": {'reduction': 'none'}}, {\"module\": 'torch.nn', \"class\": 'CrossEntropyLoss', \"params\": {'reduction': 'none'}}],\n",
    "                          \"metrics_f\": [{\"module\": 'experimenter.evaluation', \"class\": 'Dummy', \"params\": {}}, {\"module\": 'experimenter.evaluation', \"class\": 'Accuracy', \"params\": {}}]}},\n",
    "            \"optimizer\":{\n",
    "                \"module\": \"torch.optim\",\n",
    "                \"class\": \"Adam\",\n",
    "                \"params\":{\n",
    "                    \"params\": {\n",
    "                    \"eval\": 1,\n",
    "                    \"value\": \"config['model']['model'].parameters()\"},\n",
    "                'lr':  0.001}}\n",
    "           }\n",
    "\n",
    "#unlabeled_data = [{'inp':[\"يالت رجل\", \"قال رجلي\"],'label':[['agree']],'mask':[1]}]*10\n",
    "\n",
    "config =copy.deepcopy(multi_configs)\n",
    "\n",
    "# Initialize two models\n",
    "trainer_smart = training.BasicTrainer(config)\n",
    "\n",
    "res_smart = trainer_smart.train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Number of OOV: 2\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "\n",
    "def test_ecoder_freezing():\n",
    "    enc = experimenter.utils.text.encoder()\n",
    "    sen1 = \"This is a test sent\"\n",
    "    sen2 = \"This is a test sent!?\"  #Has 2 unseen characters\n",
    "    _ = enc(sen1)\n",
    "    len_before = len(enc.get_vocab())\n",
    "    print(len_before)\n",
    "    enc.freeze()\n",
    "    _ = enc(sen2)\n",
    "    assert len_before == len(enc.get_vocab())\n",
    "    enc.unfreeze()\n",
    "    _ = enc(sen2)\n",
    "    print(len(enc.get_vocab()))\n",
    "    assert len_before == len(enc.get_vocab()) - 2\n",
    "    \n",
    "\n",
    "test_ecoder_freezing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8669170476812541]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on training\n",
    "selected_data = trainer_smart.processor.get_data()\n",
    "\n",
    "trainer_smart._evaluate_batches(selected_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.75]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.75]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.75]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.8]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.7916666666666666]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.7857142857142857]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.75]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6944444444444444]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.725]\n",
      "[array(1., dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.7045454545454546]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6875]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6730769230769231]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6785714285714286]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.7]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6875]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6764705882352942]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6666666666666666]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6578947368421053]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.675]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6785714285714286]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6704545454545454]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6521739130434783]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6354166666666666]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.64]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6442307692307693]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6481481481481481]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6339285714285714]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.646551724137931]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6416666666666667]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6451612903225806]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.640625]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6363636363636364]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6397058823529411]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.65]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6527777777777778]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6486486486486487]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6447368421052632]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6474358974358975]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6375]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6402439024390244]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6428571428571429]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.627906976744186]\n",
      "[array(0., dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6363636363636364]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6388888888888888]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6467391304347826]\n",
      "[array(1., dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6542553191489362]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6614583333333334]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6581632653061225]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.66]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6568627450980392]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6490384615384616]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6462264150943396]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6481481481481481]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6409090909090909]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6428571428571429]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6403508771929824]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6379310344827587]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6398305084745762]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6416666666666667]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.639344262295082]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6370967741935484]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6388888888888888]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.64453125]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6423076923076924]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6439393939393939]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6492537313432836]\n",
      "[array(1., dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6507352941176471]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6521739130434783]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6571428571428571]\n",
      "[array(1., dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6584507042253521]\n",
      "[array(0.75, dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6631944444444444]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6575342465753424]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6554054054054054]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6566666666666666]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6546052631578947]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6525974025974026]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6538461538461539]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6518987341772152]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.653125]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6574074074074074]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6585365853658537]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6506024096385542]\n",
      "[array(0., dtype=float32)]\n",
      "0.0\n",
      "===================\n",
      "[0.6517857142857143]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6529411764705882]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6540697674418605]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6522988505747126]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6534090909090909]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6544943820224719]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6527777777777778]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6538461538461539]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6521739130434783]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6505376344086021]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6542553191489362]\n",
      "[array(1., dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6552631578947369]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.65625]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.654639175257732]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6556122448979592]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6540404040404041]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.655]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.655940594059406]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6568627450980392]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6601941747572816]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6634615384615384]\n",
      "[array(1., dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6642857142857143]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6627358490566038]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6635514018691588]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6643518518518519]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6651376146788991]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6659090909090909]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6621621621621622]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6629464285714286]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.665929203539823]\n",
      "[array(1., dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6622807017543859]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.658695652173913]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6616379310344828]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6645299145299145]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.663135593220339]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6596638655462185]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.0\n",
      "===================\n",
      "[0.6583333333333333]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6570247933884298]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6577868852459017]\n",
      "[array(0.75, dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "===================\n",
      "[0.6565040650406504]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.655241935483871]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.0\n",
      "===================\n",
      "[0.656]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6587301587301587]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6594488188976378]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.66015625]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6608527131782945]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6596153846153846]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6603053435114504]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6609848484848485]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6616541353383458]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.664179104477612]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6611111111111111]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.65625]\n",
      "[array(0., dtype=float32)]\n",
      "0.0\n",
      "===================\n",
      "[0.656934306569343]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6557971014492754]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6528776978417267]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6535714285714286]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6524822695035462]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.653169014084507]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6520979020979021]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6545138888888888]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.656896551724138]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6558219178082192]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6547619047619048]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.0\n",
      "===================\n",
      "[0.6554054054054054]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6543624161073825]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6566666666666666]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6572847682119205]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.65625]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6535947712418301]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6525974025974026]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6516129032258065]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6490384615384616]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6464968152866242]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6455696202531646]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6446540880503144]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.64375]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6459627329192547]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6481481481481481]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6487730061349694]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.649390243902439]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.646969696969697]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6460843373493976]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6467065868263473]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6458333333333334]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6464497041420119]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6470588235294118]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6461988304093568]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6468023255813954]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6473988439306358]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6479885057471264]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6457142857142857]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6477272727272727]\n",
      "[array(1., dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6497175141242938]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6502808988764045]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6480446927374302]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6458333333333334]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6464088397790055]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6483516483516484]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6502732240437158]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6494565217391305]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6472972972972973]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6478494623655914]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6443850267379679]\n",
      "[array(0., dtype=float32)]\n",
      "0.0\n",
      "===================\n",
      "[0.6449468085106383]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6468253968253969]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6460526315789473]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6465968586387435]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6458333333333334]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6463730569948186]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6481958762886598]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6487179487179487]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6492346938775511]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6472081218274112]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6464646464646465]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6469849246231156]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6475]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6480099502487562]\n",
      "[array(0.75, dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6497524752475248]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6502463054187192]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6482843137254902]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6463414634146342]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6456310679611651]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6473429951690821]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6466346153846154]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.645933014354067]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6476190476190476]\n",
      "[array(1., dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.6469194312796208]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6485849056603774]\n",
      "[array(1., dtype=float32)]\n",
      "1.0\n",
      "===================\n",
      "[0.6467136150234741]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6460280373831776]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6441860465116279]\n",
      "[array(0.25, dtype=float32)]\n",
      "0.25\n",
      "===================\n",
      "[0.6435185185185185]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "[0.6440092165898618]\n",
      "[array(0.75, dtype=float32)]\n",
      "0.75\n",
      "===================\n",
      "[0.643348623853211]\n",
      "[array(0.5, dtype=float32)]\n",
      "0.5\n",
      "===================\n",
      "217 [0.643348623853211]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6376146788990825"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_smart.evaluator.reset()\n",
    "correct = 0\n",
    "total = 0\n",
    "res = []\n",
    "for j, b in enumerate(selected_data[2]):\n",
    "    #print(j)\n",
    "    decoded_batch = trainer_smart.processor.decode(trainer_smart.processor._from_batch(b), list_input=True)\n",
    "    #print(decoded_batch)\n",
    "    res.append(trainer_smart.model(b))\n",
    "    val_loss = trainer_smart.evaluator.update_batch(res[j])\n",
    "    print(val_loss)\n",
    "    print(trainer_smart.evaluator.get_metrics(res[j]))\n",
    "    pred = trainer_smart.predict(decoded_batch, decode=False)\n",
    "    current_corrects = sum([p['pred'][0] == p['label'][0] for p in pred])\n",
    "    current_total = len(pred)\n",
    "    print(current_corrects / current_total)\n",
    "    #[print(\"||\".join([str(p['pred'][0][0]), str(p['label'][0][0])])) for p in pred]\n",
    "    print(\"===================\")\n",
    "    correct += current_corrects\n",
    "    total += current_total\n",
    "print(j, val_loss)\n",
    "correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.3403890160183066]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data = trainer_smart.processor.get_data()\n",
    "#pred = trainer_smart._evaluate_batches(unlabeled_data[0])\n",
    "pred = [p['pred'][1] == p['label'][1] for p in pred]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42660550458715596"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for b in selected_data[0]:\n",
    "    decoded_batch = trainer_smart.processor.decode(trainer_smart.processor._from_batch(b), list_input=True)\n",
    "    pred = trainer_smart.predict(decoded_batch, decode=False)\n",
    "    correct += sum([p['pred'][0] == p['label'][0] for p in pred])\n",
    "    total += len(pred)\n",
    "\n",
    "#sum(pred) / len(pred) \n",
    "correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]])\n",
      "tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1]])\n",
      "2|1\n",
      "1|1\n",
      "2|0\n",
      "0|1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_smart.evaluator.metrics_f[0](res[j]['pred'][0], res[j]['label'][0])\n",
    "print(res[j]['pred'][0])\n",
    "print(res[j]['label'][0])\n",
    "[print(\"|\".join([str(p['pred'][0][0]), str(p['label'][0][0])])) for p in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[34, 13,  4, 15, 14,  6, 22, 11, 29,  2, 30,  3, 22,  6,  4,  2,  8, 15,\n",
       "         22,  4,  3, 15,  9,  6,  2,  2, 24,  2,  4,  3,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0],\n",
       "        [ 4, 34,  9, 28,  4, 28,  4,  9,  6,  4,  2,  7, 12,  4, 31, 30,  6, 21,\n",
       "          8, 11, 41,  6, 34,  5, 12,  6, 15, 24, 11,  8,  6,  3, 34,  4, 27, 26,\n",
       "          4,  9,  6,  4],\n",
       "        [ 3, 11,  8, 15, 32,  6,  9, 36, 27, 16,  6,  4,  2,  9, 10, 22, 15, 27,\n",
       "          4,  9,  6, 29,  4,  2, 28,  3,  2, 14,  6, 27, 15,  6,  9, 12, 13, 15,\n",
       "          4, 35, 35,  6],\n",
       "        [ 4,  2,  8, 11,  2,  4, 12,  6, 15, 31, 27, 32, 35, 35,  6, 11, 15,  7,\n",
       "         32, 32,  6,  3, 13,  4, 24, 29, 21,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[j]['inp'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 11,\n",
       " 8,\n",
       " 15,\n",
       " 32,\n",
       " 6,\n",
       " 9,\n",
       " 36,\n",
       " 27,\n",
       " 16,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 10,\n",
       " 22,\n",
       " 15,\n",
       " 27,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 29,\n",
       " 4,\n",
       " 2,\n",
       " 28,\n",
       " 3,\n",
       " 2,\n",
       " 14,\n",
       " 6,\n",
       " 27,\n",
       " 15,\n",
       " 6,\n",
       " 9,\n",
       " 12,\n",
       " 13,\n",
       " 15,\n",
       " 4,\n",
       " 35,\n",
       " 35,\n",
       " 0]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]['inp'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res[j]['inp'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data = trainer_smart.processor.upload_data()\n",
    "pred = trainer_smart.predict(unlabeled_data[:1000], decode=False)\n",
    "pred = [p['pred'][0] == p['label'][0] for p in pred]\n",
    "sum(pred) / len(pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "1.4463678059057392\n",
      "Remaining data before / after filtering the learned batch\n",
      "5052\n",
      "4852\n",
      "Starting training:\n",
      "Epoch: 11: Train loss (last batch): 1.0623605251312256, validation loss: 1.0581321716308594\n",
      "Best model saved at: model.state\n",
      "Epoch: 12: Train loss (last batch): 0.7871716618537903, validation loss: 1.0428305864334106\n",
      "Best model saved at: model.state\n",
      "Epoch: 13: Train loss (last batch): 1.1186940670013428, validation loss: 1.0667556524276733\n",
      "Epoch: 14: Train loss (last batch): 1.3207297325134277, validation loss: 1.088484525680542\n",
      "Epoch: 15: Train loss (last batch): 1.049099326133728, validation loss: 1.0591458082199097\n",
      "Test loss: 1.0410854816436768\n",
      "Starting training:\n",
      "Epoch: 11: Train loss (last batch): 0.9175735712051392, validation loss: 1.0657908916473389\n",
      "Best model saved at: model.state\n",
      "Epoch: 12: Train loss (last batch): 1.0900952816009521, validation loss: 1.0391796827316284\n",
      "Best model saved at: model.state\n",
      "Epoch: 13: Train loss (last batch): 0.9136359691619873, validation loss: 1.0532846450805664\n",
      "Epoch: 14: Train loss (last batch): 0.916773796081543, validation loss: 1.1083197593688965\n",
      "Epoch: 15: Train loss (last batch): 0.9679549336433411, validation loss: 1.062324047088623\n",
      "Test loss: 1.0580006837844849\n",
      "step 1\n",
      "1.38206908893295\n",
      "Remaining data before / after filtering the learned batch\n",
      "4852\n",
      "4652\n",
      "Starting training:\n",
      "Epoch: 16: Train loss (last batch): 1.1189956665039062, validation loss: 1.0685073137283325\n",
      "Best model saved at: model.state\n",
      "Epoch: 17: Train loss (last batch): 0.9065014123916626, validation loss: 1.056923747062683\n",
      "Best model saved at: model.state\n",
      "Epoch: 18: Train loss (last batch): 1.2434639930725098, validation loss: 1.0566821098327637\n",
      "Best model saved at: model.state\n",
      "Epoch: 19: Train loss (last batch): 0.9182854890823364, validation loss: 1.0448863506317139\n",
      "Best model saved at: model.state\n",
      "Epoch: 20: Train loss (last batch): 1.1181001663208008, validation loss: 1.0406386852264404\n",
      "Best model saved at: model.state\n",
      "Test loss: 1.0372388362884521\n",
      "Starting training:\n",
      "Epoch: 16: Train loss (last batch): 0.9485210180282593, validation loss: 1.055065393447876\n",
      "Best model saved at: model.state\n",
      "Epoch: 17: Train loss (last batch): 0.7836827635765076, validation loss: 1.0603671073913574\n",
      "Epoch: 18: Train loss (last batch): 0.6406785845756531, validation loss: 1.0549569129943848\n",
      "Best model saved at: model.state\n",
      "Epoch: 19: Train loss (last batch): 0.8027253150939941, validation loss: 1.070547342300415\n",
      "Epoch: 20: Train loss (last batch): 0.7191056609153748, validation loss: 1.1182302236557007\n",
      "Test loss: 1.1129086017608643\n",
      "step 2\n",
      "1.4244890975058873\n",
      "Remaining data before / after filtering the learned batch\n",
      "4652\n",
      "4452\n",
      "Starting training:\n",
      "Epoch: 21: Train loss (last batch): 0.887336015701294, validation loss: 1.0367839336395264\n",
      "Best model saved at: model.state\n",
      "Epoch: 22: Train loss (last batch): 1.700872778892517, validation loss: 1.0353449583053589\n",
      "Best model saved at: model.state\n",
      "Epoch: 23: Train loss (last batch): 1.0598227977752686, validation loss: 1.037508487701416\n",
      "Epoch: 24: Train loss (last batch): 0.6425461769104004, validation loss: 1.0609575510025024\n",
      "Epoch: 25: Train loss (last batch): 0.8147274851799011, validation loss: 1.057183027267456\n",
      "Test loss: 1.0293245315551758\n",
      "Starting training:\n",
      "Epoch: 21: Train loss (last batch): 1.581282615661621, validation loss: 1.0483531951904297\n",
      "Best model saved at: model.state\n",
      "Epoch: 22: Train loss (last batch): 1.1523157358169556, validation loss: 1.0490975379943848\n",
      "Epoch: 23: Train loss (last batch): 0.728576123714447, validation loss: 1.0555427074432373\n",
      "Epoch: 24: Train loss (last batch): 0.9184376001358032, validation loss: 1.0605363845825195\n",
      "Epoch: 25: Train loss (last batch): 1.4234167337417603, validation loss: 1.1444568634033203\n",
      "Test loss: 1.1540645360946655\n",
      "step 3\n",
      "1.3539831780538807\n",
      "Remaining data before / after filtering the learned batch\n",
      "4452\n",
      "4252\n",
      "Starting training:\n",
      "Epoch: 26: Train loss (last batch): 1.0146806240081787, validation loss: 1.0480296611785889\n",
      "Best model saved at: model.state\n",
      "Epoch: 27: Train loss (last batch): 0.8028432130813599, validation loss: 1.0411734580993652\n",
      "Best model saved at: model.state\n",
      "Epoch: 28: Train loss (last batch): 0.8426177501678467, validation loss: 1.0547993183135986\n",
      "Epoch: 29: Train loss (last batch): 1.3315198421478271, validation loss: 1.0865635871887207\n",
      "Epoch: 30: Train loss (last batch): 0.632735550403595, validation loss: 1.0776888132095337\n",
      "Test loss: 1.0302448272705078\n",
      "Starting training:\n",
      "Epoch: 26: Train loss (last batch): 1.2227954864501953, validation loss: 1.0606178045272827\n",
      "Best model saved at: model.state\n",
      "Epoch: 27: Train loss (last batch): 0.8665452003479004, validation loss: 1.088058352470398\n",
      "Epoch: 28: Train loss (last batch): 0.8043904304504395, validation loss: 1.071440577507019\n",
      "Epoch: 29: Train loss (last batch): 1.3654265403747559, validation loss: 1.0667750835418701\n",
      "Epoch: 30: Train loss (last batch): 0.683922529220581, validation loss: 1.0966410636901855\n",
      "Test loss: 1.1328626871109009\n",
      "step 4\n",
      "1.3022707130495343\n",
      "Remaining data before / after filtering the learned batch\n",
      "4252\n",
      "4052\n",
      "Starting training:\n",
      "Epoch: 31: Train loss (last batch): 1.4126191139221191, validation loss: 1.0514800548553467\n",
      "Best model saved at: model.state\n",
      "Epoch: 32: Train loss (last batch): 0.7288591861724854, validation loss: 1.0232900381088257\n",
      "Best model saved at: model.state\n",
      "Epoch: 33: Train loss (last batch): 1.1392401456832886, validation loss: 1.0322906970977783\n",
      "Epoch: 34: Train loss (last batch): 1.0369181632995605, validation loss: 1.0408066511154175\n",
      "Epoch: 35: Train loss (last batch): 0.9922736883163452, validation loss: 1.0423856973648071\n",
      "Test loss: 1.0337517261505127\n",
      "Starting training:\n",
      "Epoch: 31: Train loss (last batch): 1.2288153171539307, validation loss: 1.0651389360427856\n",
      "Best model saved at: model.state\n",
      "Epoch: 32: Train loss (last batch): 1.1433689594268799, validation loss: 1.0513683557510376\n",
      "Best model saved at: model.state\n",
      "Epoch: 33: Train loss (last batch): 0.8266544938087463, validation loss: 1.0444252490997314\n",
      "Best model saved at: model.state\n",
      "Epoch: 34: Train loss (last batch): 1.0492926836013794, validation loss: 1.0839295387268066\n",
      "Epoch: 35: Train loss (last batch): 0.820056140422821, validation loss: 1.0886927843093872\n",
      "Test loss: 1.1261471509933472\n",
      "step 5\n",
      "1.3391012990209195\n",
      "Remaining data before / after filtering the learned batch\n",
      "4052\n",
      "3852\n",
      "Starting training:\n",
      "Epoch: 36: Train loss (last batch): 1.0320680141448975, validation loss: 1.0291532278060913\n",
      "Best model saved at: model.state\n",
      "Epoch: 37: Train loss (last batch): 1.2148253917694092, validation loss: 1.0175738334655762\n",
      "Best model saved at: model.state\n",
      "Epoch: 38: Train loss (last batch): 0.9755659103393555, validation loss: 1.0366709232330322\n",
      "Epoch: 39: Train loss (last batch): 0.6312243342399597, validation loss: 1.03802490234375\n",
      "Epoch: 40: Train loss (last batch): 1.0244741439819336, validation loss: 1.081958532333374\n",
      "Test loss: 1.070029377937317\n",
      "Starting training:\n",
      "Epoch: 36: Train loss (last batch): 1.4246127605438232, validation loss: 1.0604970455169678\n",
      "Best model saved at: model.state\n",
      "Epoch: 37: Train loss (last batch): 0.7153793573379517, validation loss: 1.0429418087005615\n",
      "Best model saved at: model.state\n",
      "Epoch: 38: Train loss (last batch): 0.8627102971076965, validation loss: 1.0565204620361328\n",
      "Epoch: 39: Train loss (last batch): 0.8533084392547607, validation loss: 1.0458333492279053\n",
      "Epoch: 40: Train loss (last batch): 0.5927283763885498, validation loss: 1.054093837738037\n",
      "Test loss: 1.0683168172836304\n",
      "step 6\n",
      "1.2924677676357585\n",
      "Remaining data before / after filtering the learned batch\n",
      "3852\n",
      "3652\n",
      "Starting training:\n",
      "Epoch: 41: Train loss (last batch): 0.8066275715827942, validation loss: 1.0894639492034912\n",
      "Best model saved at: model.state\n",
      "Epoch: 42: Train loss (last batch): 0.832968533039093, validation loss: 1.059286117553711\n",
      "Best model saved at: model.state\n",
      "Epoch: 43: Train loss (last batch): 0.6269853711128235, validation loss: 1.0258805751800537\n",
      "Best model saved at: model.state\n",
      "Epoch: 44: Train loss (last batch): 0.8821024894714355, validation loss: 1.0154292583465576\n",
      "Best model saved at: model.state\n",
      "Epoch: 45: Train loss (last batch): 0.8464297652244568, validation loss: 1.0149935483932495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: model.state\n",
      "Test loss: 0.9901295900344849\n",
      "Starting training:\n",
      "Epoch: 41: Train loss (last batch): 1.2643591165542603, validation loss: 1.0451629161834717\n",
      "Best model saved at: model.state\n",
      "Epoch: 42: Train loss (last batch): 1.170201063156128, validation loss: 1.052954912185669\n",
      "Epoch: 43: Train loss (last batch): 0.956839919090271, validation loss: 1.0390136241912842\n",
      "Best model saved at: model.state\n",
      "Epoch: 44: Train loss (last batch): 1.3444180488586426, validation loss: 1.0519754886627197\n",
      "Epoch: 45: Train loss (last batch): 0.9354345798492432, validation loss: 1.061521053314209\n",
      "Test loss: 1.0727882385253906\n",
      "step 7\n",
      "1.3193084428125657\n",
      "Remaining data before / after filtering the learned batch\n",
      "3652\n",
      "3452\n",
      "Starting training:\n",
      "Epoch: 46: Train loss (last batch): 0.9979891777038574, validation loss: 1.013409972190857\n",
      "Best model saved at: model.state\n",
      "Epoch: 47: Train loss (last batch): 1.0384202003479004, validation loss: 1.0270806550979614\n",
      "Epoch: 48: Train loss (last batch): 0.9247334003448486, validation loss: 1.009764552116394\n",
      "Best model saved at: model.state\n",
      "Epoch: 49: Train loss (last batch): 0.6837278604507446, validation loss: 1.045725703239441\n",
      "Epoch: 50: Train loss (last batch): 0.789980947971344, validation loss: 1.032309651374817\n",
      "Test loss: 1.0286307334899902\n",
      "Starting training:\n",
      "Epoch: 46: Train loss (last batch): 1.3269755840301514, validation loss: 1.0603642463684082\n",
      "Best model saved at: model.state\n",
      "Epoch: 47: Train loss (last batch): 0.9808925986289978, validation loss: 1.0559049844741821\n",
      "Best model saved at: model.state\n",
      "Epoch: 48: Train loss (last batch): 0.8008478283882141, validation loss: 1.0520098209381104\n",
      "Best model saved at: model.state\n",
      "Epoch: 49: Train loss (last batch): 0.907310426235199, validation loss: 1.0645307302474976\n",
      "Epoch: 50: Train loss (last batch): 0.41877052187919617, validation loss: 1.0908708572387695\n",
      "Test loss: 1.1069509983062744\n",
      "step 8\n",
      "1.2781573650261924\n",
      "Remaining data before / after filtering the learned batch\n",
      "3452\n",
      "3252\n",
      "Starting training:\n",
      "Epoch: 51: Train loss (last batch): 1.3593906164169312, validation loss: 1.013441562652588\n",
      "Best model saved at: model.state\n",
      "Epoch: 52: Train loss (last batch): 0.9417921304702759, validation loss: 1.0099481344223022\n",
      "Best model saved at: model.state\n",
      "Epoch: 53: Train loss (last batch): 0.9773699045181274, validation loss: 1.0344018936157227\n",
      "Epoch: 54: Train loss (last batch): 0.6684784889221191, validation loss: 1.028428554534912\n",
      "Epoch: 55: Train loss (last batch): 0.46160081028938293, validation loss: 1.0423749685287476\n",
      "Test loss: 1.033412218093872\n",
      "Starting training:\n",
      "Epoch: 51: Train loss (last batch): 0.9440658092498779, validation loss: 1.0656908750534058\n",
      "Best model saved at: model.state\n",
      "Epoch: 52: Train loss (last batch): 1.1964144706726074, validation loss: 1.057700753211975\n",
      "Best model saved at: model.state\n",
      "Epoch: 53: Train loss (last batch): 0.7416738867759705, validation loss: 1.0769537687301636\n",
      "Epoch: 54: Train loss (last batch): 0.6342279314994812, validation loss: 1.0637058019638062\n",
      "Epoch: 55: Train loss (last batch): 0.7380937337875366, validation loss: 1.0628786087036133\n",
      "Test loss: 1.0676766633987427\n",
      "step 9\n",
      "1.238697168526423\n",
      "Remaining data before / after filtering the learned batch\n",
      "3252\n",
      "3052\n",
      "Starting training:\n",
      "Epoch: 56: Train loss (last batch): 1.1292697191238403, validation loss: 1.0446393489837646\n",
      "Best model saved at: model.state\n",
      "Epoch: 57: Train loss (last batch): 0.44034045934677124, validation loss: 1.0251669883728027\n",
      "Best model saved at: model.state\n",
      "Epoch: 58: Train loss (last batch): 0.5164992809295654, validation loss: 1.032751202583313\n",
      "Epoch: 59: Train loss (last batch): 0.24869348108768463, validation loss: 1.0182362794876099\n",
      "Best model saved at: model.state\n",
      "Epoch: 60: Train loss (last batch): 0.37373003363609314, validation loss: 1.0427300930023193\n",
      "Test loss: 1.0232443809509277\n",
      "Starting training:\n",
      "Epoch: 56: Train loss (last batch): 1.4621491432189941, validation loss: 1.0792131423950195\n",
      "Best model saved at: model.state\n",
      "Epoch: 57: Train loss (last batch): 0.7178587317466736, validation loss: 1.0389840602874756\n",
      "Best model saved at: model.state\n",
      "Epoch: 58: Train loss (last batch): 0.7196930646896362, validation loss: 1.095894455909729\n",
      "Epoch: 59: Train loss (last batch): 0.6102267503738403, validation loss: 1.0397205352783203\n",
      "Epoch: 60: Train loss (last batch): 1.4276756048202515, validation loss: 1.0749943256378174\n",
      "Test loss: 1.0919307470321655\n"
     ]
    }
   ],
   "source": [
    "# Predict on unlabled data\n",
    "def get_ent(preds):\n",
    "    res = []\n",
    "    for example in preds:\n",
    "        ent = 0\n",
    "        for p in example:\n",
    "            ent += -p * np.log2(p)\n",
    "        res.append(ent)\n",
    "        \n",
    "    return res\n",
    "\n",
    "import random\n",
    "top_n = 200\n",
    "num_steps = range(10)\n",
    "for step in num_steps:\n",
    "    print(\"step {}\".format(step))\n",
    "    pred = trainer_smart.predict(unlabeled_data, decode=True)\n",
    "\n",
    "    outs = [o for p in pred for o in p['meta']]\n",
    "    ent = get_ent(outs)\n",
    "    ent = np.array(ent)\n",
    "    print(ent.mean())\n",
    "    idx = (-ent).argsort()[:top_n]\n",
    "    #print(\"selected examples based on entropy: \")\n",
    "    #print(idx)\n",
    "    filtered = [unlabeled_data[s] for s in idx]\n",
    "    random_d = [unlabeled_data[s] for s in random.sample(range(len(unlabeled_data)), top_n)]\n",
    "\n",
    "    # Update remaining\n",
    "    print(\"Remaining data before / after filtering the learned batch\")\n",
    "    print(len(unlabeled_data))\n",
    "    unlabeled_data = [s for i, s in enumerate(unlabeled_data) if i not in idx]\n",
    "    print(len(unlabeled_data))\n",
    "\n",
    "    #print(random)\n",
    "    #print(filtered)\n",
    "\n",
    "    #Retrain\n",
    "    filtered_encoded = trainer_smart.processor(filtered, data_type='full', list_input= True, as_batches=True)\n",
    "    random_encoded = trainer_smart.processor(random_d, data_type='full', list_input= True, as_batches=True)\n",
    "    res_smart = trainer_smart.train_model([filtered_encoded, val_data, test_data])\n",
    "    res_random = trainer_random.train_model([random_encoded, val_data, test_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'other'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent.argmax()\n",
    "ent[idx[100]]\n",
    "ent[ent.argmax()]\n",
    "unlabeled_data[idx[100]]['label'][0][0]\n",
    "trainer_random.predict([unlabeled_data[idx[100]], unlabeled_data[idx[100]]])[0]['pred'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'during_training': {'1': {'train_loss': 1.0959333181381226,\n",
       "   'val_loss': 1.0971243381500244},\n",
       "  '2': {'train_loss': 1.1127394437789917, 'val_loss': 1.0927706956863403},\n",
       "  '3': {'train_loss': 1.2162951231002808, 'val_loss': 1.0798826217651367},\n",
       "  '4': {'train_loss': 1.1723943948745728, 'val_loss': 1.0638456344604492},\n",
       "  '5': {'train_loss': 1.0028953552246094, 'val_loss': 1.0499619245529175},\n",
       "  '6': {'train_loss': 1.090139627456665, 'val_loss': 1.0430418252944946},\n",
       "  '7': {'train_loss': 0.9486572742462158, 'val_loss': 1.050020694732666},\n",
       "  '8': {'train_loss': 1.1977633237838745, 'val_loss': 1.0444697141647339},\n",
       "  '9': {'train_loss': 1.0246580839157104, 'val_loss': 1.0404095649719238},\n",
       "  '10': {'train_loss': 0.6556094288825989, 'val_loss': 1.0469732284545898},\n",
       "  '11': {'train_loss': 1.0623605251312256, 'val_loss': 1.0581321716308594},\n",
       "  '12': {'train_loss': 0.7871716618537903, 'val_loss': 1.0428305864334106},\n",
       "  '13': {'train_loss': 1.1186940670013428, 'val_loss': 1.0667556524276733},\n",
       "  '14': {'train_loss': 1.3207297325134277, 'val_loss': 1.088484525680542},\n",
       "  '15': {'train_loss': 1.049099326133728, 'val_loss': 1.0591458082199097},\n",
       "  '16': {'train_loss': 1.1189956665039062, 'val_loss': 1.0685073137283325},\n",
       "  '17': {'train_loss': 0.9065014123916626, 'val_loss': 1.056923747062683},\n",
       "  '18': {'train_loss': 1.2434639930725098, 'val_loss': 1.0566821098327637},\n",
       "  '19': {'train_loss': 0.9182854890823364, 'val_loss': 1.0448863506317139},\n",
       "  '20': {'train_loss': 1.1181001663208008, 'val_loss': 1.0406386852264404},\n",
       "  '21': {'train_loss': 0.887336015701294, 'val_loss': 1.0367839336395264},\n",
       "  '22': {'train_loss': 1.700872778892517, 'val_loss': 1.0353449583053589},\n",
       "  '23': {'train_loss': 1.0598227977752686, 'val_loss': 1.037508487701416},\n",
       "  '24': {'train_loss': 0.6425461769104004, 'val_loss': 1.0609575510025024},\n",
       "  '25': {'train_loss': 0.8147274851799011, 'val_loss': 1.057183027267456},\n",
       "  '26': {'train_loss': 1.0146806240081787, 'val_loss': 1.0480296611785889},\n",
       "  '27': {'train_loss': 0.8028432130813599, 'val_loss': 1.0411734580993652},\n",
       "  '28': {'train_loss': 0.8426177501678467, 'val_loss': 1.0547993183135986},\n",
       "  '29': {'train_loss': 1.3315198421478271, 'val_loss': 1.0865635871887207},\n",
       "  '30': {'train_loss': 0.632735550403595, 'val_loss': 1.0776888132095337},\n",
       "  '31': {'train_loss': 1.4126191139221191, 'val_loss': 1.0514800548553467},\n",
       "  '32': {'train_loss': 0.7288591861724854, 'val_loss': 1.0232900381088257},\n",
       "  '33': {'train_loss': 1.1392401456832886, 'val_loss': 1.0322906970977783},\n",
       "  '34': {'train_loss': 1.0369181632995605, 'val_loss': 1.0408066511154175},\n",
       "  '35': {'train_loss': 0.9922736883163452, 'val_loss': 1.0423856973648071},\n",
       "  '36': {'train_loss': 1.0320680141448975, 'val_loss': 1.0291532278060913},\n",
       "  '37': {'train_loss': 1.2148253917694092, 'val_loss': 1.0175738334655762},\n",
       "  '38': {'train_loss': 0.9755659103393555, 'val_loss': 1.0366709232330322},\n",
       "  '39': {'train_loss': 0.6312243342399597, 'val_loss': 1.03802490234375},\n",
       "  '40': {'train_loss': 1.0244741439819336, 'val_loss': 1.081958532333374},\n",
       "  '41': {'train_loss': 0.8066275715827942, 'val_loss': 1.0894639492034912},\n",
       "  '42': {'train_loss': 0.832968533039093, 'val_loss': 1.059286117553711},\n",
       "  '43': {'train_loss': 0.6269853711128235, 'val_loss': 1.0258805751800537},\n",
       "  '44': {'train_loss': 0.8821024894714355, 'val_loss': 1.0154292583465576},\n",
       "  '45': {'train_loss': 0.8464297652244568, 'val_loss': 1.0149935483932495},\n",
       "  '46': {'train_loss': 0.9979891777038574, 'val_loss': 1.013409972190857},\n",
       "  '47': {'train_loss': 1.0384202003479004, 'val_loss': 1.0270806550979614},\n",
       "  '48': {'train_loss': 0.9247334003448486, 'val_loss': 1.009764552116394},\n",
       "  '49': {'train_loss': 0.6837278604507446, 'val_loss': 1.045725703239441},\n",
       "  '50': {'train_loss': 0.789980947971344, 'val_loss': 1.032309651374817},\n",
       "  '51': {'train_loss': 1.3593906164169312, 'val_loss': 1.013441562652588},\n",
       "  '52': {'train_loss': 0.9417921304702759, 'val_loss': 1.0099481344223022},\n",
       "  '53': {'train_loss': 0.9773699045181274, 'val_loss': 1.0344018936157227},\n",
       "  '54': {'train_loss': 0.6684784889221191, 'val_loss': 1.028428554534912},\n",
       "  '55': {'train_loss': 0.46160081028938293, 'val_loss': 1.0423749685287476},\n",
       "  '56': {'train_loss': 1.1292697191238403, 'val_loss': 1.0446393489837646},\n",
       "  '57': {'train_loss': 0.44034045934677124, 'val_loss': 1.0251669883728027},\n",
       "  '58': {'train_loss': 0.5164992809295654, 'val_loss': 1.032751202583313},\n",
       "  '59': {'train_loss': 0.24869348108768463, 'val_loss': 1.0182362794876099},\n",
       "  '60': {'train_loss': 0.37373003363609314, 'val_loss': 1.0427300930023193}},\n",
       " 'best': {'train_loss': 0.24869348108768463, 'val_loss': 1.0182362794876099},\n",
       " 'test': {tensor(1.0232, grad_fn=<DivBackward0>)}}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_smart['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'during_training': {'1': {'train_loss': 0.9932723045349121,\n",
       "   'val_loss': 1.1460973024368286},\n",
       "  '2': {'train_loss': 1.8655883073806763, 'val_loss': 0.9856749773025513},\n",
       "  '3': {'train_loss': 0.5568922758102417, 'val_loss': 0.9643186330795288},\n",
       "  '4': {'train_loss': 0.8071258664131165, 'val_loss': 0.9565287828445435},\n",
       "  '5': {'train_loss': 0.5423922538757324, 'val_loss': 0.9556002020835876},\n",
       "  '6': {'train_loss': 0.9349740743637085, 'val_loss': 0.9569883942604065},\n",
       "  '7': {'train_loss': 1.3021361827850342, 'val_loss': 0.9552294015884399},\n",
       "  '8': {'train_loss': 0.726514458656311, 'val_loss': 0.9544289708137512},\n",
       "  '9': {'train_loss': 1.0989142656326294, 'val_loss': 0.955574095249176},\n",
       "  '10': {'train_loss': 0.4896788001060486, 'val_loss': 0.955385684967041},\n",
       "  '11': {'train_loss': 1.331813097000122, 'val_loss': 0.955652117729187},\n",
       "  '12': {'train_loss': 1.322219729423523, 'val_loss': 0.956355631351471},\n",
       "  '13': {'train_loss': 0.5277446508407593, 'val_loss': 0.9499547481536865},\n",
       "  '14': {'train_loss': 0.9116512537002563, 'val_loss': 0.9504325985908508},\n",
       "  '15': {'train_loss': 0.9161804914474487, 'val_loss': 0.9494937062263489},\n",
       "  '16': {'train_loss': 0.7597208619117737, 'val_loss': 0.9517405033111572},\n",
       "  '17': {'train_loss': 0.8987411260604858, 'val_loss': 0.9483364820480347},\n",
       "  '18': {'train_loss': 0.719573974609375, 'val_loss': 0.947390079498291},\n",
       "  '19': {'train_loss': 0.9619226455688477, 'val_loss': 0.9482828378677368},\n",
       "  '20': {'train_loss': 0.5232361555099487, 'val_loss': 0.9488431811332703},\n",
       "  '21': {'train_loss': 1.098631739616394, 'val_loss': 0.9484428763389587},\n",
       "  '22': {'train_loss': 0.9191515445709229, 'val_loss': 0.9475398063659668},\n",
       "  '23': {'train_loss': 0.9256869554519653, 'val_loss': 0.9500771164894104},\n",
       "  '24': {'train_loss': 0.7456627488136292, 'val_loss': 0.9495823979377747},\n",
       "  '25': {'train_loss': 0.5184981226921082, 'val_loss': 0.951298177242279},\n",
       "  '26': {'train_loss': 1.0890486240386963, 'val_loss': 0.9519614577293396},\n",
       "  '27': {'train_loss': 1.1691080331802368, 'val_loss': 0.9534572958946228},\n",
       "  '28': {'train_loss': 0.8661308884620667, 'val_loss': 0.9515878558158875},\n",
       "  '29': {'train_loss': 1.0989185571670532, 'val_loss': 0.9469677805900574},\n",
       "  '30': {'train_loss': 0.9079446196556091, 'val_loss': 0.9465218782424927},\n",
       "  '31': {'train_loss': 1.2639210224151611, 'val_loss': 0.945889413356781},\n",
       "  '32': {'train_loss': 0.7375331521034241, 'val_loss': 0.9458656311035156},\n",
       "  '33': {'train_loss': 0.5814820528030396, 'val_loss': 0.9450486302375793},\n",
       "  '34': {'train_loss': 0.5351881980895996, 'val_loss': 0.9469635486602783},\n",
       "  '35': {'train_loss': 1.0820927619934082, 'val_loss': 0.9447499513626099},\n",
       "  '36': {'train_loss': 1.0759968757629395, 'val_loss': 0.9451717734336853},\n",
       "  '37': {'train_loss': 0.9299224615097046, 'val_loss': 0.9432560801506042},\n",
       "  '38': {'train_loss': 0.7116551995277405, 'val_loss': 0.944229781627655},\n",
       "  '39': {'train_loss': 0.9142112731933594, 'val_loss': 0.9436624646186829},\n",
       "  '40': {'train_loss': 0.8875399231910706, 'val_loss': 0.9461364150047302},\n",
       "  '41': {'train_loss': 1.1118757724761963, 'val_loss': 0.9531173706054688},\n",
       "  '42': {'train_loss': 1.0626928806304932, 'val_loss': 0.9455386996269226},\n",
       "  '43': {'train_loss': 0.8824523091316223, 'val_loss': 0.9486888647079468},\n",
       "  '44': {'train_loss': 0.6935596466064453, 'val_loss': 0.9515203237533569},\n",
       "  '45': {'train_loss': 0.9065508842468262, 'val_loss': 0.9455541372299194},\n",
       "  '46': {'train_loss': 1.103534460067749, 'val_loss': 0.9430764317512512},\n",
       "  '47': {'train_loss': 0.6294281482696533, 'val_loss': 0.9455287456512451},\n",
       "  '48': {'train_loss': 0.9337501525878906, 'val_loss': 0.9440619349479675}},\n",
       " 'best': {'train_loss': 1.103534460067749, 'val_loss': 0.9430764317512512},\n",
       " 'test': {tensor(0.9291, grad_fn=<DivBackward0>)}}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_random['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0958, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = trainer.evaluate(selected_data)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will create train, dev, test(s) splits\n",
      "Total params: 2245\n",
      "Starting training:\n",
      "Epoch: 0: Train loss (last batch): 4.0922346115112305, validation loss: 4.063948154449463\n",
      "Best model saved at: model.state\n",
      "Epoch: 2: Train loss (last batch): 1.541860818862915, validation loss: 1.414445400238037\n",
      "Best model saved at: model.state\n",
      "tensor(1.3859, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "!python ../run.py --config_file \"../local/pairs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'experimenter.utils.text' has no attribute 'chainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-9fa6e7f3fa72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_chainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-9fa6e7f3fa72>\u001b[0m in \u001b[0;36mtest_chainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuncs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"مرحبا هنا\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'experimenter.utils.text' has no attribute 'chainer'"
     ]
    }
   ],
   "source": [
    "def test_chainer():\n",
    "    tokenizer = text.tokenizer(sep=' ')\n",
    "    enc = text.encoder(update_vocab=True)\n",
    "    chain = text.chainer(funcs=[tokenizer, enc, enc.decode, tokenizer.detokenize])\n",
    "    \n",
    "    inp = \"مرحبا هنا\"\n",
    "    assert inp == chain(inp, list_input=False)\n",
    "    \n",
    "    inp = [\"hi_there man_\", \"how are you?\"]\n",
    "    assert inp == chain(inp, list_input=True)\n",
    "\n",
    "test_chainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
