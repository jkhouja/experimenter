{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cc9b5c370353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mtrainer_smart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0mres_smart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_smart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/repo/latynt/experimenter/experimenter/utils/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mtloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mtloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pair stance classification\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import importlib\n",
    "import numpy as np\n",
    "sys.path.append(\"..\")\n",
    "import copy\n",
    "\n",
    "\n",
    "import experimenter\n",
    "from experimenter import evaluation\n",
    "from experimenter.utils import modeling, training, data, text, utils\n",
    "importlib.reload(experimenter)\n",
    "importlib.reload(text)\n",
    "importlib.reload(modeling)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(training)\n",
    "importlib.reload(data)\n",
    "importlib.reload(utils)\n",
    "\n",
    "import logging\n",
    "\n",
    "configs = {\"epochs\": 12,\n",
    "            \"experiment_name\": \"PairStanceClassification\",\n",
    "             \"root_path\" : \"/Users/jkhouja/workspace/experiments/arabic_media/\",\n",
    "             \"experiment_output_file\": \"results.json\",\n",
    "             \"model_path\": \"model.state\",\n",
    "             \"log_interval\": 1, \n",
    "             \"disable_gpu\": False,\n",
    "           \"processor\": {\n",
    "                \"module\": \"experimenter.utils.data\",\n",
    "                \"class\": \"PairStanceProvider\",\n",
    "                \"params\":{\"input_path\": \"data/batch_0_to_15000_pairs_sep__score_10_ngrams_2_3_4_5_6.csv\",\n",
    "                          \"seq_len\": {'inp':[40,40], 'label':[1],'mask':[1]}, \"batch_size\": 4, \"splits\": [.7, .2, .1], \"drop_last\":True,  \"shuffle\": True, \"vocab_path\": \"vocab.json\"}},\n",
    "           \"model\":{\n",
    "                \"module\": \"experimenter.utils.modeling\",\n",
    "                \"class\": \"RNNPairModel\",\n",
    "                \"params\":{\"embedding_dim\": 10, \"hidden_dim\": 60, \"num_classes\": 3, \"dropout\": 0, \"max_seq_len\": {\"eval\": 1, \"value\": \"config['processor']['params']['seq_len']['inp'][0]\"}}},\n",
    "           \n",
    "            \"evaluator\":{\n",
    "                \"module\": \"experimenter.evaluation\",\n",
    "                \"class\": \"ListEvaluator\",\n",
    "                \"params\": {\"loss_f\": [{\"module\": 'torch.nn', \"class\": 'CrossEntropyLoss', \"params\": {'reduction': 'none'}}],\n",
    "                          \"metrics_f\": [{\"module\": 'experimenter.evaluation', \"class\": 'Accuracy', \"params\": {}}]}},\n",
    "            \"optimizer\":{\n",
    "                \"module\": \"torch.optim\",\n",
    "                \"class\": \"Adam\",\n",
    "                \"params\":{\n",
    "                    \"params\": {\n",
    "                    \"eval\": 1,\n",
    "                    \"value\": \"config['model']['model'].parameters()\"},\n",
    "                'lr':  0.001}}\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "lm_configs = {\"epochs\": 5,\n",
    "            \"experiment_name\": \"LanguageModeling\",\n",
    "             \"root_path\" : \"/Users/jkhouja/workspace/experiments/arabic_media/\",\n",
    "             \"experiment_output_file\": \"results.json\",\n",
    "             \"model_path\": \"model.state\",\n",
    "             \"log_interval\": 1, \n",
    "             \"disable_gpu\": False,\n",
    "           \"processor\": {\n",
    "                \"module\": \"experimenter.utils.data\",\n",
    "                \"class\": \"LMProvider\",\n",
    "                \"params\":{\"input_path\": \"/Users/jkhouja/workspace/repo/arabic_media/data/pairs/batch_0_to_15000_pairs_eq_weight_random.csv\",\n",
    "                          \"seq_len\": {'inp':[50], 'label':[50],'mask':[1]}, \"batch_size\": 4, \"splits\": [.1, .2, .1, .6], \"drop_last\":\"False\",  \"shuffle\": \"True\", \"vocab_path\": \"local/vocab.json\"}},\n",
    "           \"model\":{\n",
    "                \"module\": \"experimenter.utils.modeling\",\n",
    "                \"class\": \"RNNLMModel\",\n",
    "                \"params\":{\"embedding_dim\": 8, \"hidden_dim\": 30, \"dropout\": 0, \"max_seq_len\": {\"eval\": 1, \"value\": \"config['processor']['params']['seq_len']['inp'][0]\"}}},\n",
    "           \n",
    "            \"evaluator\":{\n",
    "                \"module\": \"experimenter.evaluation\",\n",
    "                \"class\": \"ListEvaluator\",\n",
    "                \"params\": {\"loss_f\": [{\"module\": 'torch.nn', \"class\": 'CrossEntropyLoss', \"params\": {'reduction': 'none'}}]}},\n",
    "            \"optimizer\":{\n",
    "                \"module\": \"torch.optim\",\n",
    "                \"class\": \"Adam\",\n",
    "                \"params\":{\n",
    "                    \"params\": {\n",
    "                    \"eval\": 1,\n",
    "                    \"value\": \"config['model']['model'].parameters()\"},\n",
    "                'lr':  0.001}}\n",
    "           }\n",
    "\n",
    "multi_configs = {\"epochs\": 25,\n",
    "           \"experiment_name\": \"MultiTaskModeling\",\n",
    "             \"root_path\" : \"/Users/jkhouja/workspace/experiments/arabic_media/\",\n",
    "             \"experiment_output_file\": \"results.json\",\n",
    "             \"model_path\": \"model.state\",\n",
    "             \"log_interval\": 1, \n",
    "             \"disable_gpu\": False,\n",
    "           \"processor\": {\n",
    "                \"module\": \"experimenter.utils.data\",\n",
    "                \"class\": \"MultiLMPairProvider\",\n",
    "                \"params\":{\"input_path\": \"/Users/jkhouja/workspace/repo/arabic_media/data/for training/batch_0_to_15000_pairs_eq_weight_random.csv\",\n",
    "                          \"seq_len\": {'inp':[50, 50], 'label':[50, 1],'mask':[1, 1]}, \"batch_size\": 4, \"splits\": [.7, .2, .1], \"drop_last\":True,  \"shuffle\": True, \"vocab_path\": \"local/vocab.json\"}},\n",
    "           \"model\":{\n",
    "                \"module\": \"experimenter.utils.modeling\",\n",
    "                \"class\": \"RNNMultiLMPairModel\",\n",
    "                \"params\":{\"embedding_dim\": 20, \"hidden_dim\": 100, \"dropout\": 0, \"lm_classes\": {\"eval\": 1, \"value\":\"config['processor']['params']['vocab_size']\"},  \"num_classes\":3 , \"max_seq_len\": {\"eval\": 1, \"value\": \"config['processor']['params']['seq_len']['inp'][0]\"}}},\n",
    "           \n",
    "            \"evaluator\":{\n",
    "                \"module\": \"experimenter.evaluation\",\n",
    "                \"class\": \"ListEvaluator\",\n",
    "                \"params\": {\"loss_f\": [{\"module\": 'torch.nn', \"class\": 'CrossEntropyLoss', \"params\": {'reduction': 'none'}}, {\"module\": 'torch.nn', \"class\": 'CrossEntropyLoss', \"params\": {'reduction': 'none'}}],\n",
    "                          \"metrics_f\": [{\"module\": 'experimenter.evaluation', \"class\": 'Dummy', \"params\": {}}, {\"module\": 'experimenter.evaluation', \"class\": 'Accuracy', \"params\": {}}]}},\n",
    "            \"optimizer\":{\n",
    "                \"module\": \"torch.optim\",\n",
    "                \"class\": \"Adam\",\n",
    "                \"params\":{\n",
    "                    \"params\": {\n",
    "                    \"eval\": 1,\n",
    "                    \"value\": \"config['model']['model'].parameters()\"},\n",
    "                'lr':  0.001}}\n",
    "           }\n",
    "\n",
    "#unlabeled_data = [{'inp':[\"يالت رجل\", \"قال رجلي\"],'label':[['agree']],'mask':[1]}]*10\n",
    "\n",
    "\n",
    "\n",
    "# Initialize two models\n",
    "\n",
    "logger = logging.getLogger()\n",
    "config =copy.deepcopy(multi_configs)\n",
    "trainer_smart = training.BasicTrainer(config)\n",
    "\n",
    "res_smart = trainer_smart.train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.6030372305682561]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on train\n",
    "selected_data = trainer_smart.processor.get_data()\n",
    "trainer_smart._evaluate_batches(selected_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will create train, dev, test(s) splits\n",
      "Total params: 59104\n",
      "Starting training:\n",
      "Epoch: 1: Train loss (last batch): 195.17881774902344, validation metrics: 0.0,0.465675057208238\n",
      "Epoch: 2: Train loss (last batch): 142.82186889648438, validation metrics: 0.0,0.5606407322654462\n",
      "Epoch: 3: Train loss (last batch): 168.8651123046875, validation metrics: 0.0,0.6344393592677345\n",
      "Epoch: 4: Train loss (last batch): 108.60322570800781, validation metrics: 0.0,0.6767734553775744\n",
      "Epoch: 5: Train loss (last batch): 155.85281372070312, validation metrics: 0.0,0.6842105263157895\n",
      "Epoch: 6: Train loss (last batch): 131.98126220703125, validation metrics: 0.0,0.6956521739130435\n",
      "Epoch: 7: Train loss (last batch): 119.71443176269531, validation metrics: 0.0,0.7105263157894737\n",
      "Epoch: 8: Train loss (last batch): 105.26661682128906, validation metrics: 0.0,0.7254004576659039\n",
      "Epoch: 9: Train loss (last batch): 108.87906646728516, validation metrics: 0.0,0.7465675057208238\n",
      "Epoch: 10: Train loss (last batch): 124.81642150878906, validation metrics: 0.0,0.7505720823798627\n",
      "Epoch: 11: Train loss (last batch): 96.14080810546875, validation metrics: 0.0,0.7683066361556065\n",
      "Epoch: 12: Train loss (last batch): 114.95059967041016, validation metrics: 0.0,0.7648741418764302\n",
      "Epoch: 13: Train loss (last batch): 84.3133316040039, validation metrics: 0.0,0.782608695652174\n",
      "Epoch: 14: Train loss (last batch): 113.74781799316406, validation metrics: 0.0,0.7711670480549199\n",
      "Epoch: 15: Train loss (last batch): 87.24824523925781, validation metrics: 0.0,0.7774599542334096\n",
      "Epoch: 16: Train loss (last batch): 136.718994140625, validation metrics: 0.0,0.7751716247139588\n",
      "Epoch: 17: Train loss (last batch): 117.43855285644531, validation metrics: 0.0,0.7723112128146453\n",
      "Epoch: 18: Train loss (last batch): 115.5284652709961, validation metrics: 0.0,0.7837528604118993\n",
      "Epoch: 19: Train loss (last batch): 101.33720397949219, validation metrics: 0.0,0.7740274599542334\n",
      "Epoch: 20: Train loss (last batch): 77.30821228027344, validation metrics: 0.0,0.7791762013729977\n",
      "Epoch: 21: Train loss (last batch): 74.99717712402344, validation metrics: 0.0,0.7911899313501144\n",
      "Epoch: 22: Train loss (last batch): 85.11326599121094, validation metrics: 0.0,0.7929061784897025\n",
      "Epoch: 23: Train loss (last batch): 90.87210083007812, validation metrics: 0.0,0.7734553775743707\n",
      "Epoch: 24: Train loss (last batch): 77.43362426757812, validation metrics: 0.0,0.7963386727688787\n",
      "Epoch: 25: Train loss (last batch): 82.13243103027344, validation metrics: 0.0,0.795766590389016\n",
      "Test metrics: 0.0,0.7889908256880734\n"
     ]
    }
   ],
   "source": [
    "# Another try to mast [1,2] - more weight on stance classification. By changing code in \n",
    "importlib.reload(training)\n",
    "importlib.reload(data)\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "config = copy.deepcopy(multi_configs)\n",
    "\n",
    "trainer_smart = training.BasicTrainer(config)\n",
    "\n",
    "res_smart = trainer_smart.train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will create train, dev, test(s) splits\n",
      "Total params: 59104\n",
      "Starting training:\n",
      "Epoch: 1: Train loss (last batch): 113.81739807128906, validation metrics: 0.0,0.4582857142857143\n",
      "Epoch: 2: Train loss (last batch): 137.22171020507812, validation metrics: 0.0,0.49057142857142855\n",
      "Epoch: 3: Train loss (last batch): 102.43212890625, validation metrics: 0.0,0.5391428571428571\n",
      "Epoch: 4: Train loss (last batch): 132.57327270507812, validation metrics: 0.0,0.5731428571428572\n",
      "Epoch: 5: Train loss (last batch): 96.04551696777344, validation metrics: 0.0,0.6034285714285714\n",
      "Epoch: 6: Train loss (last batch): 93.68408203125, validation metrics: 0.0,0.5945714285714285\n",
      "Epoch: 7: Train loss (last batch): 83.46720886230469, validation metrics: 0.0,0.5962857142857143\n",
      "Epoch: 8: Train loss (last batch): 113.39315032958984, validation metrics: 0.0,0.5988571428571429\n",
      "Epoch: 9: Train loss (last batch): 80.28907775878906, validation metrics: 0.0,0.5805714285714285\n",
      "Epoch: 10: Train loss (last batch): 74.12620544433594, validation metrics: 0.0,0.6014285714285714\n",
      "Epoch: 11: Train loss (last batch): 92.8117904663086, validation metrics: 0.0,0.604\n",
      "Epoch: 12: Train loss (last batch): 86.03762817382812, validation metrics: 0.0,0.6148571428571429\n",
      "Epoch: 13: Train loss (last batch): 184.99044799804688, validation metrics: 0.0,0.6037142857142858\n",
      "Epoch: 14: Train loss (last batch): 85.88841247558594, validation metrics: 0.0,0.6062857142857143\n",
      "Epoch: 15: Train loss (last batch): 72.960693359375, validation metrics: 0.0,0.6157142857142858\n",
      "Epoch: 16: Train loss (last batch): 82.7085189819336, validation metrics: 0.0,0.6151428571428571\n",
      "Epoch: 17: Train loss (last batch): 66.44715118408203, validation metrics: 0.0,0.6077142857142858\n",
      "Epoch: 18: Train loss (last batch): 78.70217895507812, validation metrics: 0.0,0.6068571428571429\n",
      "Epoch: 19: Train loss (last batch): 62.2503776550293, validation metrics: 0.0,0.6025714285714285\n",
      "Epoch: 20: Train loss (last batch): 79.5093994140625, validation metrics: 0.0,0.6097142857142858\n",
      "Epoch: 21: Train loss (last batch): 67.28872680664062, validation metrics: 0.0,0.6165714285714285\n",
      "Epoch: 22: Train loss (last batch): 86.86029052734375, validation metrics: 0.0,0.5971428571428572\n",
      "Epoch: 23: Train loss (last batch): 82.97845458984375, validation metrics: 0.0,0.6128571428571429\n",
      "Epoch: 24: Train loss (last batch): 83.51547241210938, validation metrics: 0.0,0.6105714285714285\n",
      "Epoch: 25: Train loss (last batch): 55.36824035644531, validation metrics: 0.0,0.6057142857142858\n",
      "Test metrics: 0.0,0.6155606407322655\n"
     ]
    }
   ],
   "source": [
    "# Another try with adding s2 to LM part with 0 mask on stance\n",
    "importlib.reload(training)\n",
    "importlib.reload(data)\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "config = copy.deepcopy(multi_configs)\n",
    "\n",
    "trainer_smart = training.BasicTrainer(config)\n",
    "\n",
    "res_smart = trainer_smart.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will create train, dev, test(s) splits\n",
      "Total params: 59104\n",
      "Starting training:\n",
      "Epoch: 1: Train loss (last batch): 209.2574462890625, validation metrics: 0.0,0.5828571428571429\n",
      "Epoch: 2: Train loss (last batch): 144.80616760253906, validation metrics: 0.0,0.6908571428571428\n",
      "Epoch: 3: Train loss (last batch): 133.73956298828125, validation metrics: 0.0,0.7285714285714285\n",
      "Epoch: 4: Train loss (last batch): 143.59291076660156, validation metrics: 0.0,0.7768571428571428\n",
      "Epoch: 5: Train loss (last batch): 112.75733947753906, validation metrics: 0.0,0.8194285714285714\n",
      "Epoch: 6: Train loss (last batch): 103.4307632446289, validation metrics: 0.0,0.8394285714285714\n",
      "Epoch: 7: Train loss (last batch): 101.30254364013672, validation metrics: 0.0,0.8665714285714285\n",
      "Epoch: 8: Train loss (last batch): 100.20878601074219, validation metrics: 0.0,0.8842857142857142\n",
      "Epoch: 9: Train loss (last batch): 96.01273345947266, validation metrics: 0.0,0.8848571428571429\n",
      "Epoch: 10: Train loss (last batch): 90.12982177734375, validation metrics: 0.0,0.8925714285714286\n",
      "Epoch: 11: Train loss (last batch): 130.30023193359375, validation metrics: 0.0,0.8945714285714286\n",
      "Epoch: 12: Train loss (last batch): 87.51852416992188, validation metrics: 0.0,0.9097142857142857\n",
      "Epoch: 13: Train loss (last batch): 98.065673828125, validation metrics: 0.0,0.9145714285714286\n",
      "Epoch: 14: Train loss (last batch): 90.85943603515625, validation metrics: 0.0,0.9157142857142857\n",
      "Epoch: 15: Train loss (last batch): 127.95738983154297, validation metrics: 0.0,0.9157142857142857\n",
      "Epoch: 16: Train loss (last batch): 90.61790466308594, validation metrics: 0.0,0.9211428571428572\n",
      "Epoch: 17: Train loss (last batch): 93.3116455078125, validation metrics: 0.0,0.9225714285714286\n",
      "Epoch: 18: Train loss (last batch): 77.623779296875, validation metrics: 0.0,0.9125714285714286\n",
      "Epoch: 19: Train loss (last batch): 103.66831970214844, validation metrics: 0.0,0.9308571428571428\n",
      "Epoch: 20: Train loss (last batch): 153.06988525390625, validation metrics: 0.0,0.9334285714285714\n",
      "Epoch: 21: Train loss (last batch): 71.57659912109375, validation metrics: 0.0,0.9274285714285714\n",
      "Epoch: 22: Train loss (last batch): 85.0821762084961, validation metrics: 0.0,0.936\n",
      "Epoch: 23: Train loss (last batch): 67.5463638305664, validation metrics: 0.0,0.9317142857142857\n",
      "Epoch: 24: Train loss (last batch): 179.0238800048828, validation metrics: 0.0,0.9194285714285715\n",
      "Epoch: 25: Train loss (last batch): 90.92344665527344, validation metrics: 0.0,0.9337142857142857\n",
      "Test metrics: 0.0,0.9364988558352403\n"
     ]
    }
   ],
   "source": [
    "# Another try with adding s2 to LM part with 0 mask on stance\n",
    "importlib.reload(training)\n",
    "importlib.reload(data)\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "config = copy.deepcopy(multi_configs)\n",
    "\n",
    "trainer_smart = training.BasicTrainer(config)\n",
    "\n",
    "res_smart = trainer_smart.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will create train, dev, test(s) splits\n",
      "Total params: 59104\n",
      "Starting training:\n",
      "Epoch: 1: Train loss (last batch): 122.08667755126953, validation metrics: 0.0,0.3438215102974828\n",
      "Epoch: 2: Train loss (last batch): 94.79762268066406, validation metrics: 0.0,0.33695652173913043\n",
      "Epoch: 3: Train loss (last batch): 95.37557220458984, validation metrics: 0.0,0.33466819221967964\n",
      "Epoch: 4: Train loss (last batch): 75.7430648803711, validation metrics: 0.0,0.33352402745995424\n",
      "Epoch: 5: Train loss (last batch): 68.06087493896484, validation metrics: 0.0,0.3306636155606407\n",
      "Epoch: 6: Train loss (last batch): 69.52253723144531, validation metrics: 0.0,0.31864988558352403\n",
      "Epoch: 7: Train loss (last batch): 77.9913558959961, validation metrics: 0.0,0.3295194508009153\n",
      "Epoch: 8: Train loss (last batch): 72.39769744873047, validation metrics: 0.0,0.3232265446224256\n",
      "Epoch: 9: Train loss (last batch): 67.80549621582031, validation metrics: 0.0,0.3169336384439359\n",
      "Epoch: 10: Train loss (last batch): 77.77806091308594, validation metrics: 0.0,0.3295194508009153\n",
      "Epoch: 11: Train loss (last batch): 68.55192565917969, validation metrics: 0.0,0.3232265446224256\n",
      "Epoch: 12: Train loss (last batch): 51.957271575927734, validation metrics: 0.0,0.3255148741418764\n",
      "Epoch: 13: Train loss (last batch): 73.85535430908203, validation metrics: 0.0,0.32151029748283755\n",
      "Epoch: 14: Train loss (last batch): 78.7459716796875, validation metrics: 0.0,0.31922196796338675\n",
      "Epoch: 15: Train loss (last batch): 71.68235778808594, validation metrics: 0.0,0.324370709382151\n",
      "Epoch: 16: Train loss (last batch): 56.217628479003906, validation metrics: 0.0,0.3255148741418764\n",
      "Epoch: 17: Train loss (last batch): 61.51604080200195, validation metrics: 0.0,0.33123569794050345\n",
      "Epoch: 18: Train loss (last batch): 59.91465377807617, validation metrics: 0.0,0.3329519450800915\n",
      "Epoch: 19: Train loss (last batch): 61.48692321777344, validation metrics: 0.0,0.34439359267734554\n",
      "Epoch: 20: Train loss (last batch): 56.01768112182617, validation metrics: 0.0,0.3266590389016018\n",
      "Epoch: 21: Train loss (last batch): 55.91352844238281, validation metrics: 0.0,0.3306636155606407\n",
      "Epoch: 22: Train loss (last batch): 66.91951751708984, validation metrics: 0.0,0.34096109839816935\n",
      "Epoch: 23: Train loss (last batch): 44.19464111328125, validation metrics: 0.0,0.33466819221967964\n",
      "Epoch: 24: Train loss (last batch): 59.87329864501953, validation metrics: 0.0,0.33237986270022885\n",
      "Epoch: 25: Train loss (last batch): 54.57345199584961, validation metrics: 0.0,0.33981693363844395\n",
      "Test metrics: 0.0,0.31995412844036697\n"
     ]
    }
   ],
   "source": [
    "# putting all stance weight to zero\n",
    "importlib.reload(training)\n",
    "importlib.reload(data)\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "config = copy.deepcopy(multi_configs)\n",
    "\n",
    "trainer_smart = training.BasicTrainer(config)\n",
    "\n",
    "res_smart = trainer_smart.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will create train, dev, test(s) splits\n",
      "Total params: 58807\n",
      "Starting training:\n",
      "Epoch: 1: Train loss (last batch): 167.3178253173828, validation metrics: 0.0,0.44142857142857145\n",
      "Epoch: 2: Train loss (last batch): 156.54412841796875, validation metrics: 0.0,0.48028571428571426\n",
      "Epoch: 3: Train loss (last batch): 101.2757568359375, validation metrics: 0.0,0.5262857142857142\n",
      "Epoch: 4: Train loss (last batch): 115.44377136230469, validation metrics: 0.0,0.5597142857142857\n",
      "Epoch: 5: Train loss (last batch): 104.06786346435547, validation metrics: 0.0,0.5888571428571429\n",
      "Epoch: 6: Train loss (last batch): 146.06484985351562, validation metrics: 0.0,0.6022857142857143\n",
      "Epoch: 7: Train loss (last batch): 67.07022094726562, validation metrics: 0.0,0.6191428571428571\n",
      "Epoch: 8: Train loss (last batch): 111.83159637451172, validation metrics: 0.0,0.6354285714285715\n",
      "Epoch: 9: Train loss (last batch): 85.94422912597656, validation metrics: 0.0,0.6362857142857142\n",
      "Epoch: 10: Train loss (last batch): 102.94386291503906, validation metrics: 0.0,0.6451428571428571\n",
      "Epoch: 11: Train loss (last batch): 112.00311279296875, validation metrics: 0.0,0.6528571428571428\n",
      "Epoch: 12: Train loss (last batch): 60.24554443359375, validation metrics: 0.0,0.6611428571428571\n",
      "Epoch: 13: Train loss (last batch): 102.88294982910156, validation metrics: 0.0,0.6654285714285715\n",
      "Epoch: 14: Train loss (last batch): 75.39490509033203, validation metrics: 0.0,0.6754285714285714\n",
      "Epoch: 15: Train loss (last batch): 82.62799072265625, validation metrics: 0.0,0.6757142857142857\n",
      "Epoch: 16: Train loss (last batch): 115.52301025390625, validation metrics: 0.0,0.68\n",
      "Epoch: 17: Train loss (last batch): 99.87736511230469, validation metrics: 0.0,0.6705714285714286\n",
      "Epoch: 18: Train loss (last batch): 57.2303466796875, validation metrics: 0.0,0.6782857142857143\n",
      "Epoch: 19: Train loss (last batch): 85.05880737304688, validation metrics: 0.0,0.6942857142857143\n",
      "Epoch: 20: Train loss (last batch): 69.55126190185547, validation metrics: 0.0,0.6917142857142857\n",
      "Epoch: 21: Train loss (last batch): 92.18203735351562, validation metrics: 0.0,0.6894285714285714\n",
      "Epoch: 22: Train loss (last batch): 74.68156433105469, validation metrics: 0.0,0.6931428571428572\n",
      "Epoch: 23: Train loss (last batch): 60.69038391113281, validation metrics: 0.0,0.692\n",
      "Epoch: 24: Train loss (last batch): 64.27326965332031, validation metrics: 0.0,0.6954285714285714\n",
      "Epoch: 25: Train loss (last batch): 56.39360046386719, validation metrics: 0.0,0.694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-177d64aaff0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrainer_smart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mres_smart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_smart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/repo/latynt/experimenter/experimenter/utils/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mtloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mtloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/repo/latynt/experimenter/experimenter/utils/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_batch, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0ms1_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO: Move length calculations to preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0ms1_packed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0ms1_all_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_last_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_packed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0ms1_all_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_all_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_packed\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\u001b[0;32m--> 529\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# putting all stance weight to 150 and s2 LM data\n",
    "importlib.reload(training)\n",
    "importlib.reload(data)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(modeling)\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "config = copy.deepcopy(multi_configs)\n",
    "\n",
    "config['epochs'] = 60\n",
    "\n",
    "trainer_smart = training.BasicTrainer(config)\n",
    "\n",
    "res_smart = trainer_smart.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All loaded data size:8754\n",
      "Will create train, dev, test(s) splits\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'encoder' object has no attribute 'freeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fc8d3b1288b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrainer_smart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mres_smart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_smart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/repo/latynt/experimenter/experimenter/utils/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mconfig\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/repo/latynt/experimenter/experimenter/utils/utils.py\u001b[0m in \u001b[0;36mload_class\u001b[0;34m(module_name, class_name, class_param, pass_params_as_dict)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpass_params_as_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclass_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mclass_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/repo/latynt/experimenter/experimenter/utils/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;31m#d = self._create_splits(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'encoder' object has no attribute 'freeze'"
     ]
    }
   ],
   "source": [
    "# Update code to split before pairs.  0.0,0.6291428571428571\n",
    "importlib.reload(training)\n",
    "importlib.reload(data)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(modeling)\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "config = copy.deepcopy(multi_configs)\n",
    "\n",
    "config['epochs'] = 20\n",
    "\n",
    "trainer_smart = training.BasicTrainer(config)\n",
    "\n",
    "res_smart = trainer_smart.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['disagree'], ['disagree'], ['disagree'], ['disagree']]\n",
      "[1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#unlabeled_data = trainer_smart.processor.upload_data()\n",
    "s1 = 'فتح تحقيق في قضية موت مريم مصطفى في مدينة باري'\n",
    "s2 = s1\n",
    "data2 = [{'inp': [s1, s2], 'label':[s1, [1]], 'mask':[1, 1]}] * 4\n",
    "pred = trainer_smart.predict(data2, decode=False)\n",
    "#data\n",
    "print([trainer_smart.processor.encoder['label'][1]._funcs[0].decode(p['pred'][1]) for p in pred])\n",
    "print([p['meta'][1] for p in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sمصر تدعم الناقص في سد النهض</td>\n",
       "      <td>Sألمانيا تساهم في تعويض النقص بسد النهضة الأثيوب</td>\n",
       "      <td>disagree</td>\n",
       "      <td>agree</td>\n",
       "      <td>0.496307</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sرئيس الوزراء الأردنى يرفض الاستقالة و يواصل ع...</td>\n",
       "      <td>Sاستقالة رئيس الوزراء الأردنى بعد الاحتجاجات ا...</td>\n",
       "      <td>disagree</td>\n",
       "      <td>agree</td>\n",
       "      <td>0.366510</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Sمن الذين يشملهم قرار مصادرة أملاك رموز حكم صدام</td>\n",
       "      <td>Sالجزائر: الصراع متواصل بين رئيس البرلمان ونوا...</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>-0.113336</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Sمصر تتراجع في التصنيف الائتمان</td>\n",
       "      <td>Sتحقيق أفضل تصنيف ائتماني لمصر في ٧ أعوا</td>\n",
       "      <td>disagree</td>\n",
       "      <td>agree</td>\n",
       "      <td>0.409297</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sارتفاع على النفط عالميا والسبب الخام الأميرك</td>\n",
       "      <td>Sالخام الاميركي يسبب انخفاضا في اسعار النف</td>\n",
       "      <td>disagree</td>\n",
       "      <td>disagree</td>\n",
       "      <td>0.609051</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>Sتوقعات خفض أسعار الفائدة يهبط بالدولا</td>\n",
       "      <td>Sرفع أسعار الفائدة يرفع الدولار لأعلى مستو</td>\n",
       "      <td>disagree</td>\n",
       "      <td>agree</td>\n",
       "      <td>0.460125</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>Sأردوغان يهاجم قانون القومية اليهودية ويصف إسر...</td>\n",
       "      <td>Sدعوة للعمل 4 أيام بالأسبوع فقط.. مع راتب أعل</td>\n",
       "      <td>other</td>\n",
       "      <td>agree</td>\n",
       "      <td>0.239946</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>Sاستهداف مطار في طرابلس بهجوم صاورخى و اجبار ل...</td>\n",
       "      <td>Sهجوم صاروخي يستهدف مطار في طرابلس ويجبر ليبيا...</td>\n",
       "      <td>agree</td>\n",
       "      <td>agree</td>\n",
       "      <td>0.290395</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>Sالنفط يهوي لضعف الطل</td>\n",
       "      <td>Sطلب قوي يرفع أسعار النف</td>\n",
       "      <td>disagree</td>\n",
       "      <td>agree</td>\n",
       "      <td>0.421226</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>Sاعتراف قطري بالخسائر الاقتصادية جراء الحصا</td>\n",
       "      <td>Sقطر تعترف بخسائرها الفادحة بسبب المقاطع</td>\n",
       "      <td>agree</td>\n",
       "      <td>agree</td>\n",
       "      <td>0.250501</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0                         Sمصر تدعم الناقص في سد النهض   \n",
       "1    Sرئيس الوزراء الأردنى يرفض الاستقالة و يواصل ع...   \n",
       "2     Sمن الذين يشملهم قرار مصادرة أملاك رموز حكم صدام   \n",
       "3                      Sمصر تتراجع في التصنيف الائتمان   \n",
       "4        Sارتفاع على النفط عالميا والسبب الخام الأميرك   \n",
       "..                                                 ...   \n",
       "995             Sتوقعات خفض أسعار الفائدة يهبط بالدولا   \n",
       "996  Sأردوغان يهاجم قانون القومية اليهودية ويصف إسر...   \n",
       "997  Sاستهداف مطار في طرابلس بهجوم صاورخى و اجبار ل...   \n",
       "998                              Sالنفط يهوي لضعف الطل   \n",
       "999        Sاعتراف قطري بالخسائر الاقتصادية جراء الحصا   \n",
       "\n",
       "                                                     1         2         3  \\\n",
       "0     Sألمانيا تساهم في تعويض النقص بسد النهضة الأثيوب  disagree     agree   \n",
       "1    Sاستقالة رئيس الوزراء الأردنى بعد الاحتجاجات ا...  disagree     agree   \n",
       "2    Sالجزائر: الصراع متواصل بين رئيس البرلمان ونوا...     other     other   \n",
       "3             Sتحقيق أفضل تصنيف ائتماني لمصر في ٧ أعوا  disagree     agree   \n",
       "4           Sالخام الاميركي يسبب انخفاضا في اسعار النف  disagree  disagree   \n",
       "..                                                 ...       ...       ...   \n",
       "995         Sرفع أسعار الفائدة يرفع الدولار لأعلى مستو  disagree     agree   \n",
       "996      Sدعوة للعمل 4 أيام بالأسبوع فقط.. مع راتب أعل     other     agree   \n",
       "997  Sهجوم صاروخي يستهدف مطار في طرابلس ويجبر ليبيا...     agree     agree   \n",
       "998                           Sطلب قوي يرفع أسعار النف  disagree     agree   \n",
       "999           Sقطر تعترف بخسائرها الفادحة بسبب المقاطع     agree     agree   \n",
       "\n",
       "            4  correct  \n",
       "0    0.496307    False  \n",
       "1    0.366510    False  \n",
       "2   -0.113336     True  \n",
       "3    0.409297    False  \n",
       "4    0.609051     True  \n",
       "..        ...      ...  \n",
       "995  0.460125    False  \n",
       "996  0.239946    False  \n",
       "997  0.290395     True  \n",
       "998  0.421226    False  \n",
       "999  0.250501     True  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = trainer_smart.processor.decode(trainer_smart.processor.data_raw[2][0:1000], list_input=True)\n",
    "pred = trainer_smart.predict(d, decode=False)\n",
    "\n",
    "\n",
    "res = pd.DataFrame([(trainer_smart.processor.decoder['inp'][0](pred[i]['inp'][0]), trainer_smart.processor.decoder['inp'][1](pred[i]['inp'][1]), trainer_smart.processor.encoder['label'][1]._funcs[0].decode(pred[i]['label'][1])[0], trainer_smart.processor.encoder['label'][1]._funcs[0].decode(pred[i]['pred'][1])[0], pred[i]['meta'][1]) for i in range(len(d))])\n",
    "\n",
    "res['correct'] = res[2] == res[3]\n",
    "print(res['correct'].sum()  / res.shape[0])\n",
    "\n",
    "res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>agree</td>\n",
       "      <td>0.443043</td>\n",
       "      <td>0.650568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>disagree</td>\n",
       "      <td>0.538146</td>\n",
       "      <td>0.398204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>other</td>\n",
       "      <td>0.078912</td>\n",
       "      <td>0.802548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 4   correct\n",
       "2                           \n",
       "agree     0.443043  0.650568\n",
       "disagree  0.538146  0.398204\n",
       "other     0.078912  0.802548"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.groupby(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30543801188468933, 0.4390051066875458, 0.2555569112300873]\n",
      "[0.4023689925670624, 0.19882814586162567, 0.39880287647247314]\n",
      "[0.4211263954639435, 0.12805327773094177, 0.45082035660743713]\n",
      "[0.40903475880622864, 0.024560557678341866, 0.5664046406745911]\n",
      "[0.36775481700897217, 0.29465559124946594, 0.33758965134620667]\n",
      "[0.41963261365890503, 0.13530725240707397, 0.445060133934021]\n",
      "[0.34926971793174744, 0.3398396372795105, 0.31089067459106445]\n",
      "[0.42179811000823975, 0.12456441670656204, 0.4536374807357788]\n",
      "[0.41483062505722046, 0.15566132962703705, 0.4295080304145813]\n",
      "[0.4199265241622925, 0.13392706215381622, 0.4461464583873749]\n",
      "[0.3944302797317505, 0.22286103665828705, 0.38270875811576843]\n",
      "[0.4183809459209442, 0.03715715929865837, 0.5444619655609131]\n",
      "[0.38935214281082153, 0.23740479350090027, 0.3732430934906006]\n",
      "[0.4252322316169739, 0.1023598313331604, 0.4724079370498657]\n",
      "[0.23391839861869812, 0.586922824382782, 0.1791587918996811]\n",
      "[0.3276577293872833, 0.3898504078388214, 0.28249186277389526]\n",
      "[0.37854254245758057, 0.2668296694755554, 0.3546277582645416]\n",
      "[0.3831305503845215, 0.2545612156391144, 0.3623082637786865]\n",
      "[0.36170443892478943, 0.3097468316555023, 0.32854875922203064]\n",
      "[0.3352338671684265, 0.3726021647453308, 0.2921639382839203]\n",
      "[0.36173155903816223, 0.3096798062324524, 0.328588604927063]\n",
      "[0.191363126039505, 0.6696048378944397, 0.13903197646141052]\n",
      "[0.40123724937438965, 0.2023717164993286, 0.3963909447193146]\n",
      "[0.254122793674469, 0.5464183688163757, 0.1994587779045105]\n",
      "[0.31671491265296936, 0.41430413722991943, 0.268981009721756]\n",
      "[0.4108949303627014, 0.17034859955310822, 0.41875654458999634]\n",
      "[0.3582100570201874, 0.31832030415534973, 0.3234696686267853]\n",
      "[0.4049263000488281, 0.19064463675022125, 0.40442904829978943]\n",
      "[0.4182569980621338, 0.03692391514778137, 0.5448191165924072]\n",
      "[0.42269623279571533, 0.11960749328136444, 0.45769616961479187]\n",
      "[0.23997193574905396, 0.5748782157897949, 0.1851498931646347]\n",
      "[0.3763233721256256, 0.27266302704811096, 0.3510136008262634]\n",
      "[0.40792346000671387, 0.18069247901439667, 0.41138410568237305]\n",
      "[0.21210864186286926, 0.6297159790992737, 0.15817540884017944]\n",
      "[0.25184690952301025, 0.551025390625, 0.19712774455547333]\n",
      "[0.41963621973991394, 0.13529032468795776, 0.4450733959674835]\n",
      "[0.35974887013435364, 0.31455695629119873, 0.3256942629814148]\n",
      "[0.3747786581516266, 0.2766878604888916, 0.3485334813594818]\n",
      "[0.3873527944087982, 0.24299031496047974, 0.36965689063072205]\n",
      "[0.2584488093852997, 0.5376289486885071, 0.20392222702503204]\n",
      "[0.39156168699264526, 0.23114319145679474, 0.37729504704475403]\n",
      "[0.27807897329330444, 0.49717459082603455, 0.22474639117717743]\n",
      "[0.4120233952999115, 0.16626815497875214, 0.42170846462249756]\n",
      "[0.39246606826782227, 0.22855164110660553, 0.3789823055267334]\n",
      "[0.42637965083122253, 0.08972865343093872, 0.48389169573783875]\n",
      "[0.42655789852142334, 0.08643399178981781, 0.48700812458992004]\n",
      "[0.37483957409858704, 0.2765296399593353, 0.34863075613975525]\n",
      "[0.35082197189331055, 0.3361431658267975, 0.31303486227989197]\n",
      "[0.4105871915817261, 0.026150504127144814, 0.563262403011322]\n",
      "[0.3320107161998749, 0.37997424602508545, 0.2880150377750397]\n",
      "[0.41009172797203064, 0.17319831252098083, 0.4167099595069885]\n",
      "[0.3908998966217041, 0.23302893340587616, 0.37607118487358093]\n",
      "[0.36985307931900024, 0.2893419563770294, 0.3408049941062927]\n",
      "[0.40903910994529724, 0.1768706738948822, 0.4140901565551758]\n",
      "[0.31035304069519043, 0.428297221660614, 0.26134970784187317]\n",
      "[0.3617832660675049, 0.3095521330833435, 0.32866451144218445]\n",
      "[0.42275649309158325, 0.11926101893186569, 0.45798251032829285]\n",
      "[0.27209576964378357, 0.5096083879470825, 0.2182959020137787]\n",
      "[0.4204365909099579, 0.13147975504398346, 0.44808363914489746]\n",
      "[0.32559192180633545, 0.394506573677063, 0.27990156412124634]\n",
      "[0.3478491008281708, 0.3432088792324066, 0.308942049741745]\n",
      "[0.42473161220550537, 0.10637819021940231, 0.46889016032218933]\n",
      "[0.09022565186023712, 0.8537939786911011, 0.05598036199808121]\n",
      "[0.39366456866264343, 0.22508974373340607, 0.3812456429004669]\n",
      "[0.4193756580352783, 0.13649767637252808, 0.444126695394516]\n",
      "[0.31418514251708984, 0.4198870360851288, 0.26592785120010376]\n",
      "[0.1326279640197754, 0.778607964515686, 0.08876404911279678]\n",
      "[0.3997149169445038, 0.207070991396904, 0.3932141661643982]\n",
      "[0.2825250029563904, 0.4878729283809662, 0.22960209846496582]\n",
      "[0.38128072023391724, 0.2595433294773102, 0.35917598009109497]\n",
      "[0.3692352771759033, 0.2909109890460968, 0.3398537337779999]\n",
      "[0.4092620015144348, 0.024784523993730545, 0.5659534931182861]\n",
      "[0.281444251537323, 0.49013909697532654, 0.22841671109199524]\n",
      "[0.4090481102466583, 0.024573586881160736, 0.5663783550262451]\n",
      "[0.36782872676849365, 0.29446905851364136, 0.3377021849155426]\n",
      "[0.425668329000473, 0.09836719930171967, 0.47596439719200134]\n",
      "[0.22864565253257751, 0.5973532795906067, 0.17400111258029938]\n",
      "[0.40117132663726807, 0.20257681608200073, 0.3962517976760864]\n",
      "[0.41584545373916626, 0.15163354575634003, 0.4325209856033325]\n",
      "[0.4241742789745331, 0.11039043962955475, 0.46543532609939575]\n",
      "[0.4184489846229553, 0.14067472517490387, 0.440876305103302]\n",
      "[0.42454415559768677, 0.05480855703353882, 0.5206472277641296]\n",
      "[0.4083685278892517, 0.1791762411594391, 0.412455290555954]\n",
      "[0.40903496742248535, 0.024560730904340744, 0.5664042830467224]\n",
      "[0.3515651822090149, 0.3343677222728729, 0.31406712532043457]\n",
      "[0.31489697098731995, 0.4183185398578644, 0.2667844295501709]\n",
      "[0.1524474322795868, 0.742454469203949, 0.10509809851646423]\n",
      "[0.4260772466659546, 0.09390997886657715, 0.48001280426979065]\n",
      "[0.39641839265823364, 0.21700800955295563, 0.3865736424922943]\n",
      "[0.3575005829334259, 0.3200491666793823, 0.3224502503871918]\n",
      "[0.4258350431919098, 0.09666004776954651, 0.4775049388408661]\n",
      "[0.14875958859920502, 0.7492294907569885, 0.10201086848974228]\n",
      "[0.4090491831302643, 0.02457461878657341, 0.5663762092590332]\n",
      "[0.33446335792541504, 0.3743692934513092, 0.29116737842559814]\n",
      "[0.14343416690826416, 0.7589744329452515, 0.09759142249822617]\n",
      "[0.38996854424476624, 0.23566767573356628, 0.37436380982398987]\n",
      "[0.2944939434528351, 0.46255046129226685, 0.2429555505514145]\n",
      "[0.4116458296775818, 0.027321457862854004, 0.561032772064209]\n",
      "[0.37310853600502014, 0.2810083031654358, 0.34588322043418884]\n",
      "[0.23601320385932922, 0.5827633738517761, 0.18122349679470062]\n",
      "[0.3907250463962555, 0.23352564871311188, 0.3757493197917938]\n",
      "[0.3697682023048401, 0.2895577847957611, 0.3406740427017212]\n",
      "[0.4122615456581116, 0.1653948426246643, 0.4223436117172241]\n",
      "[0.3815315067768097, 0.25887084007263184, 0.35959771275520325]\n",
      "[0.3942253887653351, 0.22345863282680511, 0.3823159635066986]\n",
      "[0.4095013737678528, 0.025023650377988815, 0.5654749274253845]\n",
      "[0.41698989272117615, 0.14693941175937653, 0.4360707402229309]\n",
      "[0.11894159764051437, 0.8032031655311584, 0.07785528153181076]\n",
      "[0.06310726702213287, 0.9002062678337097, 0.036686453968286514]\n",
      "[0.3597571849822998, 0.31453651189804077, 0.3257063329219818]\n",
      "[0.3823716938495636, 0.2566109597682953, 0.36101728677749634]\n",
      "[0.4172920286655426, 0.035194989293813705, 0.5475130081176758]\n",
      "[0.27807900309562683, 0.49717459082603455, 0.224746435880661]\n",
      "[0.4071759283542633, 0.18321555852890015, 0.4096085727214813]\n",
      "[0.4170681834220886, 0.14661167562007904, 0.43632012605667114]\n",
      "[0.13997553288936615, 0.7652787566184998, 0.09474566578865051]\n",
      "[0.22785863280296326, 0.5989053845405579, 0.17323599755764008]\n",
      "[0.40903496742248535, 0.024560730904340744, 0.5664042830467224]\n",
      "[0.37005361914634705, 0.2888317406177521, 0.3411145806312561]\n",
      "[0.0933556854724884, 0.8483485579490662, 0.05829578638076782]\n",
      "[0.21120215952396393, 0.6314750909805298, 0.15732279419898987]\n",
      "[0.4090383052825928, 0.024563992395997047, 0.5663976669311523]\n",
      "[0.14068424701690674, 0.7639885544776917, 0.09532720595598221]\n",
      "[0.17580850422382355, 0.6990238428115845, 0.1251676231622696]\n",
      "[0.40950337052345276, 0.17525941133499146, 0.4152372181415558]\n",
      "[0.13731181621551514, 0.770121157169342, 0.0925670638680458]\n",
      "[0.15402892231941223, 0.7395423054695129, 0.10642872750759125]\n",
      "[0.35025447607040405, 0.33749639987945557, 0.312249094247818]\n",
      "[0.3540647029876709, 0.328368604183197, 0.3175666630268097]\n",
      "[0.3418942093849182, 0.35719650983810425, 0.30090925097465515]\n",
      "[0.4215971827507019, 0.12562550604343414, 0.45277735590934753]\n",
      "[0.3572738468647003, 0.32060080766677856, 0.32212528586387634]\n",
      "[0.42655640840530396, 0.08646616339683533, 0.48697736859321594]\n",
      "[0.4143448770046234, 0.15754945576190948, 0.4281056225299835]\n",
      "[0.38951948285102844, 0.23693394660949707, 0.37354663014411926]\n",
      "[0.29382064938545227, 0.46398645639419556, 0.24219292402267456]\n",
      "[0.27570584416389465, 0.5021175146102905, 0.22217659652233124]\n",
      "[0.3865666091442108, 0.24516719579696655, 0.368266224861145]\n",
      "[0.4162544310092926, 0.03348904475569725, 0.5502565503120422]\n",
      "[0.40406233072280884, 0.19343826174736023, 0.40249934792518616]\n",
      "[0.4142943322658539, 0.030621090903878212, 0.5550845265388489]\n",
      "[0.3447533845901489, 0.350506991147995, 0.3047395944595337]\n",
      "[0.3715895116329193, 0.28491082787513733, 0.34349963068962097]\n",
      "[0.4006693661212921, 0.2041335552930832, 0.3951970636844635]\n",
      "[0.3981364071369171, 0.21186774969100952, 0.3899959325790405]\n",
      "[0.33829808235168457, 0.36554428935050964, 0.2961576581001282]\n",
      "[0.23046748340129852, 0.5937556028366089, 0.1757768988609314]\n",
      "[0.40903523564338684, 0.024561041966080666, 0.5664036273956299]\n",
      "[0.4257972538471222, 0.09705836325883865, 0.47714439034461975]\n",
      "[0.10578358918428421, 0.8265584707260132, 0.06765802204608917]\n",
      "[0.2848123610019684, 0.48306605219841003, 0.23212164640426636]\n",
      "[0.388410747051239, 0.24004383385181427, 0.3715454339981079]\n",
      "[0.23505865037441254, 0.5846598148345947, 0.18028156459331512]\n",
      "[0.34245380759239197, 0.35589101910591125, 0.30165520310401917]\n",
      "[0.4177507758140564, 0.14371562004089355, 0.43853357434272766]\n",
      "[0.4134073257446289, 0.16112801432609558, 0.4254646301269531]\n",
      "[0.4104108214378357, 0.17207126319408417, 0.41751793026924133]\n",
      "[0.1272079199552536, 0.7883844971656799, 0.08440756052732468]\n",
      "[0.2810742259025574, 0.49091407656669617, 0.22801165282726288]\n",
      "[0.155363529920578, 0.7370817065238953, 0.10755481570959091]\n",
      "[0.4182891249656677, 0.141378253698349, 0.4403325319290161]\n",
      "[0.2649723291397095, 0.5242913961410522, 0.2107362151145935]\n",
      "[0.30948272347450256, 0.43019968271255493, 0.2603176236152649]\n",
      "[0.408589631319046, 0.17841878533363342, 0.4129915237426758]\n",
      "[0.3752952516078949, 0.27534493803977966, 0.34935978055000305]\n",
      "[0.20622099936008453, 0.6411145925521851, 0.15266437828540802]\n",
      "[0.41782522201538086, 0.14339540898799896, 0.438779354095459]\n",
      "[0.4090586304664612, 0.02458389475941658, 0.5663575530052185]\n",
      "[0.40966904163360596, 0.02519305795431137, 0.5651378631591797]\n",
      "[0.4236258566379547, 0.11400622129440308, 0.46236780285835266]\n",
      "[0.40955764055252075, 0.025080278515815735, 0.5653620958328247]\n",
      "[0.415668249130249, 0.1523454487323761, 0.4319863021373749]\n",
      "[0.10220404714345932, 0.8328617215156555, 0.06493423134088516]\n",
      "[0.4259704649448395, 0.09516917914152145, 0.47886037826538086]\n",
      "[0.41474565863609314, 0.031245091930031776, 0.554009199142456]\n",
      "[0.42464157938957214, 0.055280011147260666, 0.5200783610343933]\n",
      "[0.18858715891838074, 0.6748852133750916, 0.13652761280536652]\n",
      "[0.1745607703924179, 0.7013662457466125, 0.12407306581735611]\n",
      "[0.35935690999031067, 0.31551724672317505, 0.3251258432865143]\n",
      "[0.1947195678949356, 0.6632026433944702, 0.14207778871059418]\n",
      "[0.42674267292022705, 0.07773477584123611, 0.49552255868911743]\n",
      "[0.42370420694351196, 0.05120251327753067, 0.5250933170318604]\n",
      "[0.36508724093437195, 0.3013497591018677, 0.3335629999637604]\n",
      "[0.38289546966552734, 0.25519707798957825, 0.361907422542572]\n",
      "[0.42673259973526, 0.07656706869602203, 0.49670034646987915]\n",
      "[0.3984777331352234, 0.21083666384220123, 0.39068564772605896]\n",
      "[0.387636661529541, 0.24220165610313416, 0.37016165256500244]\n",
      "[0.39170902967453003, 0.23072221875190735, 0.3775687515735626]\n",
      "[0.3841369152069092, 0.25182923674583435, 0.36403384804725647]\n",
      "[0.2722213566303253, 0.5093483328819275, 0.21843035519123077]\n",
      "[0.20241671800613403, 0.6484465003013611, 0.1491367369890213]\n",
      "[0.20390184223651886, 0.6455873847007751, 0.1505107879638672]\n",
      "[0.4095463752746582, 0.0250689759850502, 0.5653846263885498]\n",
      "[0.39744833111763, 0.213935986161232, 0.3886156678199768]\n",
      "[0.35341712832450867, 0.3299270272254944, 0.31665584444999695]\n",
      "[0.18894824385643005, 0.6741991639137268, 0.13685260713100433]\n",
      "[0.3652521073818207, 0.3009379208087921, 0.3338099420070648]\n",
      "[0.3921267092227936, 0.22952628135681152, 0.3783470690250397]\n",
      "[0.1268685907125473, 0.7889950275421143, 0.08413638919591904]\n",
      "[0.4260387122631073, 0.06441544741392136, 0.5095458030700684]\n",
      "[0.42588871717453003, 0.0960814580321312, 0.4780297875404358]\n",
      "[0.38949644565582275, 0.2369987964630127, 0.37350478768348694]\n",
      "[0.40903782844543457, 0.024563521146774292, 0.5663986802101135]\n",
      "[0.4266255795955658, 0.08479510992765427, 0.48857933282852173]\n",
      "[0.2823255658149719, 0.488291472196579, 0.22938306629657745]\n",
      "[0.19488342106342316, 0.6628896594047546, 0.1422269344329834]\n",
      "[0.40904346108436584, 0.02456900291144848, 0.5663875937461853]\n",
      "[0.12340854108333588, 0.7952093482017517, 0.0813821330666542]\n",
      "[0.26958146691322327, 0.5148055553436279, 0.2156130075454712]\n",
      "[0.11417460441589355, 0.8116977214813232, 0.0741276890039444]\n",
      "[0.13817033171653748, 0.7685617804527283, 0.09326798468828201]\n",
      "[0.4157352149486542, 0.15207690000534058, 0.43218788504600525]\n",
      "[0.422875314950943, 0.11857128143310547, 0.4585534334182739]\n",
      "[0.41116639971733093, 0.026781829074025154, 0.5620518326759338]\n",
      "[0.4249102771282196, 0.05665534734725952, 0.5184344053268433]\n",
      "[0.29123228788375854, 0.46949347853660583, 0.23927424848079681]\n",
      "[0.38558700680732727, 0.24786478281021118, 0.36654818058013916]\n",
      "[0.2664099633693695, 0.5213383436203003, 0.212251678109169]\n",
      "[0.3933713436126709, 0.22593970596790314, 0.38068893551826477]\n",
      "[0.3906317353248596, 0.23379039764404297, 0.375577837228775]\n",
      "[0.2902698516845703, 0.47153592109680176, 0.23819421231746674]\n",
      "[0.22737717628479004, 0.5998542904853821, 0.17276856303215027]\n",
      "[0.41784757375717163, 0.14329908788204193, 0.43885335326194763]\n",
      "[0.4132849872112274, 0.16158902645111084, 0.42512598633766174]\n",
      "[0.4229164123535156, 0.11833058297634125, 0.45875295996665955]\n",
      "[0.3009992241859436, 0.44860267639160156, 0.2503981292247772]\n",
      "[0.33612146973609924, 0.37056291103363037, 0.29331570863723755]\n",
      "[0.40320515632629395, 0.19618037343025208, 0.40061450004577637]\n",
      "[0.3573893904685974, 0.3203197717666626, 0.32229083776474]\n",
      "[0.18007338047027588, 0.6909981369972229, 0.1289285123348236]\n",
      "[0.2296408712863922, 0.5953888297080994, 0.174970343708992]\n",
      "[0.4091198146343231, 0.02464408054947853, 0.5662360191345215]\n",
      "[0.4044507145881653, 0.19218634068965912, 0.4033629894256592]\n",
      "[0.4197424054145813, 0.13479401171207428, 0.4454635679721832]\n",
      "[0.23913487792015076, 0.576548159122467, 0.1843169629573822]\n",
      "[0.18035806715488434, 0.690461277961731, 0.1291806399822235]\n",
      "[0.36274075508117676, 0.3071848452091217, 0.33007436990737915]\n",
      "[0.3589439392089844, 0.31652775406837463, 0.324528306722641]\n",
      "[0.36111757159233093, 0.3111935257911682, 0.3276888430118561]\n",
      "[0.22905798256397247, 0.5965394973754883, 0.17440244555473328]\n",
      "[0.4142943322658539, 0.030621090903878212, 0.5550845265388489]\n",
      "[0.21643850207328796, 0.6212925910949707, 0.1622689664363861]\n",
      "[0.42653176188468933, 0.08698897063732147, 0.486479252576828]\n",
      "[0.3791148364543915, 0.26531511545181274, 0.3555700182914734]\n",
      "[0.3870580494403839, 0.2438076287508011, 0.3691342771053314]\n",
      "[0.4158162772655487, 0.15175099670886993, 0.43243271112442017]\n",
      "[0.07464970648288727, 0.8806259632110596, 0.04472431540489197]\n",
      "[0.25659340620040894, 0.5414040088653564, 0.2020025998353958]\n",
      "[0.3502514660358429, 0.3375036120414734, 0.3122449517250061]\n",
      "[0.39768731594085693, 0.21321909129619598, 0.3890935778617859]\n",
      "[0.36997511982917786, 0.2890315353870392, 0.34099334478378296]\n",
      "[0.259823203086853, 0.5348273515701294, 0.205349400639534]\n",
      "[0.13260293006896973, 0.7786533236503601, 0.08874380588531494]\n",
      "[0.4216197729110718, 0.12550689280033112, 0.4528733193874359]\n",
      "[0.4134550094604492, 0.16094806790351868, 0.4255969226360321]\n",
      "[0.41678839921951294, 0.1477787047624588, 0.43543291091918945]\n",
      "[0.26358306407928467, 0.5271404385566711, 0.2092764973640442]\n",
      "[0.4263860583305359, 0.08962532132863998, 0.48398861289024353]\n",
      "[0.38707467913627625, 0.24376168847084045, 0.3691636919975281]\n",
      "[0.41263431310653687, 0.16401857137680054, 0.42334702610969543]\n",
      "[0.3499101400375366, 0.3383164703845978, 0.3117733895778656]\n",
      "[0.34934258460998535, 0.3396663963794708, 0.31099098920822144]\n",
      "[0.4044598937034607, 0.19215668737888336, 0.4033834636211395]\n",
      "[0.3905610740184784, 0.23399098217487335, 0.3754480183124542]\n",
      "[0.4090767204761505, 0.024601638317108154, 0.5663216710090637]\n",
      "[0.33233949542045593, 0.3792246878147125, 0.28843584656715393]\n",
      "[0.1428733617067337, 0.7599980235099792, 0.09712867438793182]\n",
      "[0.4078984558582306, 0.18077732622623444, 0.41132423281669617]\n",
      "[0.24562811851501465, 0.5635548233985901, 0.19081702828407288]\n",
      "[0.38070252537727356, 0.2610904276371002, 0.3582070469856262]\n",
      "[0.3321392238140106, 0.379681259393692, 0.2881794571876526]\n",
      "[0.3254563510417938, 0.3948114216327667, 0.27973222732543945]\n",
      "[0.28578612208366394, 0.48101508617401123, 0.23319879174232483]\n",
      "[0.3598756194114685, 0.3142460286617279, 0.3258783221244812]\n",
      "[0.426646888256073, 0.08419078588485718, 0.4891623556613922]\n",
      "[0.1138860434293747, 0.8122107982635498, 0.07390324771404266]\n",
      "[0.2940885126590729, 0.463415265083313, 0.24249617755413055]\n",
      "[0.409036785364151, 0.024562517181038857, 0.5664006471633911]\n",
      "[0.41020697355270386, 0.025747811421751976, 0.564045250415802]\n",
      "[0.2841969430446625, 0.4843606948852539, 0.23144233226776123]\n",
      "[0.4129716753959656, 0.16276346147060394, 0.4242648482322693]\n",
      "[0.42405965924263, 0.11116980761289597, 0.46477052569389343]\n",
      "[0.4052719473838806, 0.18951818346977234, 0.4052099287509918]\n",
      "[0.4206075072288513, 0.04188628867268562, 0.5375061631202698]\n",
      "[0.41931474208831787, 0.1367775946855545, 0.44390764832496643]\n",
      "[0.36693018674850464, 0.2967320680618286, 0.33633777499198914]\n",
      "[0.3713368773460388, 0.28555744886398315, 0.34310564398765564]\n",
      "[0.4096660912036896, 0.17469142377376556, 0.4156424403190613]\n",
      "[0.33036550879478455, 0.38371753692626953, 0.2859169542789459]\n",
      "[0.4204062819480896, 0.13162703812122345, 0.44796663522720337]\n",
      "[0.2070797085762024, 0.639456033706665, 0.15346422791481018]\n",
      "[0.1294448971748352, 0.7843553423881531, 0.0861998051404953]\n",
      "[0.3598559498786926, 0.31429433822631836, 0.3258497416973114]\n",
      "[0.3480035364627838, 0.3428432047367096, 0.3091532588005066]\n",
      "[0.21600180864334106, 0.6221436858177185, 0.16185450553894043]\n",
      "[0.37236952781677246, 0.282910019159317, 0.3447204828262329]\n",
      "[0.296573281288147, 0.4581071138381958, 0.2453196793794632]\n",
      "[0.4203442931175232, 0.13192765414714813, 0.4477280080318451]\n",
      "[0.42649492621421814, 0.08771860599517822, 0.4857863783836365]\n",
      "[0.3938206434249878, 0.22463662922382355, 0.38154277205467224]\n",
      "[0.40186619758605957, 0.2004079669713974, 0.39772582054138184]\n",
      "[0.2002185434103012, 0.6526712775230408, 0.14711017906665802]\n",
      "[0.3947746455669403, 0.22185412049293518, 0.3833712637424469]\n",
      "[0.0824669748544693, 0.8672158718109131, 0.05031706765294075]\n",
      "[0.41708511114120483, 0.14654068648815155, 0.4363741874694824]\n",
      "[0.39218243956565857, 0.22936636209487915, 0.37845125794410706]\n",
      "[0.2816033363342285, 0.489805668592453, 0.22859099507331848]\n",
      "[0.31208354234695435, 0.42450621724128723, 0.2634102404117584]\n",
      "[0.3673815131187439, 0.2955963611602783, 0.3370221257209778]\n",
      "[0.4188178777694702, 0.1390324831008911, 0.44214969873428345]\n",
      "[0.4100233316421509, 0.1734388917684555, 0.41653770208358765]\n",
      "[0.3693029284477234, 0.29073935747146606, 0.33995771408081055]\n",
      "[0.42555874586105347, 0.09942762553691864, 0.4750136435031891]\n",
      "[0.34614232182502747, 0.3472399115562439, 0.306617796421051]\n",
      "[0.1753123551607132, 0.6999555230140686, 0.12473209947347641]\n",
      "[0.36489686369895935, 0.30182504653930664, 0.3332781195640564]\n",
      "[0.33009791374206543, 0.3843250572681427, 0.2855769991874695]\n",
      "[0.21211275458335876, 0.6297079920768738, 0.15817926824092865]\n",
      "[0.12792164087295532, 0.7870998382568359, 0.08497849106788635]\n",
      "[0.3304196000099182, 0.3835945725440979, 0.2859857678413391]\n",
      "[0.39336100220680237, 0.22596965730190277, 0.38066935539245605]\n",
      "[0.38775748014450073, 0.2418656051158905, 0.37037691473960876]\n",
      "[0.417799711227417, 0.03608604893088341, 0.5461142063140869]\n",
      "[0.18411971628665924, 0.6833553910255432, 0.13252484798431396]\n",
      "[0.3714137673377991, 0.285360723733902, 0.34322547912597656]\n",
      "[0.2845374047756195, 0.48364463448524475, 0.23181800544261932]\n",
      "[0.41964587569236755, 0.13524557650089264, 0.4451085925102234]\n",
      "[0.3639901876449585, 0.30408385396003723, 0.3319259285926819]\n",
      "[0.40908941626548767, 0.024614108726382256, 0.5662965178489685]\n",
      "[0.36300674080848694, 0.3065257966518402, 0.3304674029350281]\n",
      "[0.3222385346889496, 0.402024507522583, 0.2757369875907898]\n",
      "[0.4121236801147461, 0.02787553332746029, 0.5600007772445679]\n",
      "[0.3823716938495636, 0.2566109597682953, 0.36101728677749634]\n",
      "[0.3927573561668396, 0.22771315276622772, 0.3795294761657715]\n",
      "[0.40659230947494507, 0.18516556918621063, 0.4082421064376831]\n",
      "[0.3043949007987976, 0.44126659631729126, 0.2543385326862335]\n",
      "[0.12110986560583115, 0.7993269562721252, 0.07956314831972122]\n",
      "[0.41007205843925476, 0.025607066228985786, 0.5643208622932434]\n",
      "[0.31418508291244507, 0.41988706588745117, 0.2659277617931366]\n",
      "[0.4093881845474243, 0.024910176172852516, 0.5657016038894653]\n",
      "[0.23573090136051178, 0.5833243727684021, 0.18094469606876373]\n",
      "[0.3984661400318146, 0.21087175607681274, 0.39066216349601746]\n",
      "[0.33966153860092163, 0.3623877465724945, 0.29795077443122864]\n",
      "[0.38655588030815125, 0.24519681930541992, 0.3682473301887512]\n",
      "[0.3658023476600647, 0.29956167936325073, 0.3346359431743622]\n",
      "[0.41952940821647644, 0.1357872039079666, 0.44468340277671814]\n",
      "[0.40903937816619873, 0.024565022438764572, 0.56639564037323]\n",
      "[0.40906116366386414, 0.024586444720625877, 0.5663523077964783]\n",
      "[0.40903496742248535, 0.02456074208021164, 0.5664042830467224]\n",
      "[0.18805226683616638, 0.6759011745452881, 0.13604655861854553]\n",
      "[0.3324621915817261, 0.3789447546005249, 0.2885930836200714]\n",
      "[0.3854385018348694, 0.2482723593711853, 0.3662891387939453]\n",
      "[0.414601594209671, 0.156554713845253, 0.4288436770439148]\n",
      "[0.4090683162212372, 0.024593453854322433, 0.5663381814956665]\n",
      "[0.42332160472869873, 0.11590341478586197, 0.46077489852905273]\n",
      "[0.22940732538700104, 0.5958499312400818, 0.1747427135705948]\n",
      "[0.31246861815452576, 0.42366108298301697, 0.2638702988624573]\n",
      "[0.4211261570453644, 0.12805430591106415, 0.4508195221424103]\n",
      "[0.3808518648147583, 0.2606913149356842, 0.3584568202495575]\n",
      "[0.4162544012069702, 0.03348902240395546, 0.5502565503120422]\n",
      "[0.42641469836235046, 0.08915343880653381, 0.4844318628311157]\n",
      "[0.4237509071826935, 0.11320601403713226, 0.46304309368133545]\n",
      "[0.19634316861629486, 0.6600987911224365, 0.1435580849647522]\n",
      "[0.3644941449165344, 0.30282920598983765, 0.33267661929130554]\n",
      "[0.40824469923973083, 0.17959930002689362, 0.4121561050415039]\n",
      "[0.18342134356498718, 0.6846765279769897, 0.13190215826034546]\n",
      "[0.26607927680015564, 0.5220181345939636, 0.21190261840820312]\n",
      "[0.2799869477748871, 0.4931894540786743, 0.22682353854179382]\n",
      "[0.02587086521089077, 0.9612087607383728, 0.01292039081454277]\n",
      "[0.40905576944351196, 0.024581091478466988, 0.5663632154464722]\n",
      "[0.3063117265701294, 0.4371078610420227, 0.2565803825855255]\n",
      "[0.0590057447552681, 0.9070969820022583, 0.0338972732424736]\n",
      "[0.34167033433914185, 0.3577183187007904, 0.30061137676239014]\n",
      "[0.4235629141330719, 0.11440455913543701, 0.4620324671268463]\n",
      "[0.14351315796375275, 0.758830189704895, 0.09765664488077164]\n",
      "[0.4240536093711853, 0.11121029406785965, 0.4647360146045685]\n",
      "[0.35270562767982483, 0.3316359221935272, 0.31565847992897034]\n",
      "[0.3278225362300873, 0.38947805762290955, 0.2826994061470032]\n",
      "[0.35916370153427124, 0.315990149974823, 0.3248461186885834]\n",
      "[0.40114009380340576, 0.2026740163564682, 0.39618587493896484]\n",
      "[0.27592548727989197, 0.5016607046127319, 0.2224138081073761]\n",
      "[0.42415913939476013, 0.11049411445856094, 0.4653467833995819]\n",
      "[0.23272177577018738, 0.5892947912216187, 0.17798341810703278]\n",
      "[0.37948301434516907, 0.2643384337425232, 0.35617849230766296]\n",
      "[0.19813469052314758, 0.656668484210968, 0.14519686996936798]\n",
      "[0.3461158871650696, 0.3473021984100342, 0.30658194422721863]\n",
      "[0.4090348184108734, 0.02456059120595455, 0.5664045810699463]\n",
      "[0.314826101064682, 0.41847485303878784, 0.26669904589653015]\n",
      "[0.4148060083389282, 0.15575779974460602, 0.42943626642227173]\n",
      "[0.38740986585617065, 0.24283194541931152, 0.3697582185268402]\n",
      "[0.20597712695598602, 0.6415854096412659, 0.1524374634027481]\n",
      "[0.42662593722343445, 0.08478446304798126, 0.4885895550251007]\n",
      "[0.3479681611061096, 0.34292691946029663, 0.30910488963127136]\n",
      "[0.42314451932907104, 0.116977259516716, 0.459878146648407]\n",
      "[0.20137470960617065, 0.6504503488540649, 0.14817500114440918]\n",
      "[0.4117008447647095, 0.027384426444768906, 0.5609147548675537]\n",
      "[0.4093477427959442, 0.024869797751307487, 0.5657824873924255]\n",
      "[0.4090413749217987, 0.02456706203520298, 0.5663914680480957]\n",
      "[0.32600438594818115, 0.39357835054397583, 0.28041723370552063]\n",
      "[0.40463364124298096, 0.1915944516658783, 0.40377193689346313]\n",
      "[0.3905959725379944, 0.23389188945293427, 0.37551212310791016]\n",
      "[0.22443923354148865, 0.6056349277496338, 0.16992591321468353]\n",
      "[0.38246119022369385, 0.2563697099685669, 0.36116907000541687]\n",
      "[0.3953724205493927, 0.22009949386119843, 0.38452810049057007]\n",
      "[0.4125900864601135, 0.16418270766735077, 0.4232272505760193]\n",
      "[0.0800127238035202, 0.8714385628700256, 0.04854872077703476]\n",
      "[0.40914884209632874, 0.024672629311680794, 0.5661785006523132]\n",
      "[0.3833983242511749, 0.2538357675075531, 0.362765908241272]\n",
      "[0.37728068232536316, 0.2701541781425476, 0.35256510972976685]\n",
      "[0.4138181209564209, 0.02998385950922966, 0.5561980605125427]\n",
      "[0.33283671736717224, 0.37808993458747864, 0.2890733480453491]\n",
      "[0.30480512976646423, 0.4403775632381439, 0.25481727719306946]\n",
      "[0.3415597379207611, 0.3579759895801544, 0.3004642724990845]\n",
      "[0.3214344084262848, 0.4038199186325073, 0.2747456133365631]\n",
      "[0.24902325868606567, 0.5567249059677124, 0.19425182044506073]\n",
      "[0.3770590126514435, 0.27073612809181213, 0.3522048592567444]\n",
      "[0.3468559980392456, 0.3455565273761749, 0.30758747458457947]\n",
      "[0.4257572591304779, 0.09747259318828583, 0.47677022218704224]\n",
      "[0.4120258390903473, 0.16625909507274628, 0.42171499133110046]\n",
      "[0.2787639796733856, 0.49574509263038635, 0.2254909723997116]\n",
      "[0.09390664845705032, 0.8473882079124451, 0.0587051585316658]\n",
      "[0.42587772011756897, 0.06299693882465363, 0.5111253261566162]\n",
      "[0.32033511996269226, 0.4062700867652893, 0.27339473366737366]\n",
      "[0.3879101872444153, 0.24144048988819122, 0.3706493675708771]\n",
      "[0.4182136058807373, 0.036842841655015945, 0.544943630695343]\n",
      "[0.3805828392505646, 0.2614101469516754, 0.3580070436000824]\n",
      "[0.30344730615615845, 0.4433176815509796, 0.25323498249053955]\n",
      "[0.41603291034698486, 0.03314312919974327, 0.5508239269256592]\n",
      "[0.3450820744037628, 0.34973493218421936, 0.3051829934120178]\n",
      "[0.4090767204761505, 0.024601638317108154, 0.5663216710090637]\n",
      "[0.42624181509017944, 0.09177642315626144, 0.4819818139076233]\n",
      "[0.4090368449687958, 0.024562550708651543, 0.5664006471633911]\n",
      "[0.3907165229320526, 0.23354984819889069, 0.3757336437702179]\n",
      "[0.3515651524066925, 0.3343677520751953, 0.3140670955181122]\n",
      "[0.4098699986934662, 0.17397768795490265, 0.41615235805511475]\n",
      "[0.3533209562301636, 0.3301583230495453, 0.3165207803249359]\n",
      "[0.31999680399894714, 0.40702328085899353, 0.2729799151420593]\n",
      "[0.4045220911502838, 0.19195546209812164, 0.40352240204811096]\n",
      "[0.15547248721122742, 0.7368806600570679, 0.10764685273170471]\n",
      "[0.40903621912002563, 0.024561947211623192, 0.5664018392562866]\n",
      "[0.406604140996933, 0.18512625992298126, 0.40826961398124695]\n",
      "[0.37409016489982605, 0.2784728705883026, 0.3474370539188385]\n",
      "[0.421162486076355, 0.12786968052387238, 0.45096778869628906]\n",
      "[0.3971022963523865, 0.21497122943401337, 0.3879264295101166]\n",
      "[0.33913132548332214, 0.36361637711524963, 0.29725226759910583]\n",
      "[0.185119166970253, 0.6814634203910828, 0.13341741263866425]\n",
      "[0.3056715726852417, 0.43849822878837585, 0.2558302581310272]\n",
      "[0.39816370606422424, 0.2117852121591568, 0.3900510370731354]\n",
      "[0.40436652302742004, 0.1924581676721573, 0.4031752645969391]\n",
      "[0.1604032963514328, 0.7277635931968689, 0.11183308809995651]\n",
      "[0.30544477701187134, 0.4389903247356415, 0.2555648684501648]\n",
      "[0.2470567226409912, 0.5606840252876282, 0.19225923717021942]\n",
      "[0.41020700335502625, 0.02574782446026802, 0.564045250415802]\n",
      "[0.4105517566204071, 0.17157132923603058, 0.4178768992424011]\n",
      "[0.3204803764820099, 0.4059467911720276, 0.2735728621482849]\n",
      "[0.3643336296081543, 0.30322903394699097, 0.33243727684020996]\n",
      "[0.36614790558815, 0.29869598150253296, 0.335156112909317]\n",
      "[0.15941353142261505, 0.7295967936515808, 0.11098963767290115]\n",
      "[0.4208872616291046, 0.1292576789855957, 0.44985508918762207]\n",
      "[0.25295206904411316, 0.5487897992134094, 0.1982581913471222]\n",
      "[0.41297173500061035, 0.1627633422613144, 0.42426493763923645]\n",
      "[0.38153505325317383, 0.258861243724823, 0.3596037030220032]\n",
      "[0.4107104539871216, 0.17100687325000763, 0.41828271746635437]\n",
      "[0.37215113639831543, 0.28347086906433105, 0.3443779945373535]\n",
      "[0.25662240386009216, 0.5413451194763184, 0.20203252136707306]\n",
      "[0.27136269211769104, 0.5111253261566162, 0.21751199662685394]\n",
      "[0.3861963450908661, 0.24618856608867645, 0.3676150143146515]\n",
      "[0.3628849387168884, 0.3068277835845947, 0.33028730750083923]\n",
      "[0.3940039873123169, 0.22410351037979126, 0.38189253211021423]\n",
      "[0.41201627254486084, 0.02774958312511444, 0.5602340698242188]\n",
      "[0.3486233949661255, 0.34137409925460815, 0.31000253558158875]\n",
      "[0.34187424182891846, 0.35724306106567383, 0.3008826971054077]\n",
      "[0.4229367971420288, 0.04843958094716072, 0.5286235809326172]\n",
      "[0.41725102066993713, 0.0351247675716877, 0.5476241707801819]\n",
      "[0.4085777997970581, 0.17845961451530457, 0.4129626750946045]\n",
      "[0.4248834252357483, 0.10520880669355392, 0.4699077606201172]\n",
      "[0.21191440522670746, 0.6300930380821228, 0.15799257159233093]\n",
      "[0.3604472577571869, 0.3128426671028137, 0.3267101049423218]\n",
      "[0.16345691680908203, 0.7220976948738098, 0.11444538086652756]\n",
      "[0.37637272477149963, 0.27253401279449463, 0.3510933220386505]\n",
      "[0.39512401819229126, 0.22082960605621338, 0.3840463161468506]\n",
      "[0.28748440742492676, 0.47743159532546997, 0.2350839525461197]\n",
      "[0.3796689510345459, 0.2638446092605591, 0.3564864695072174]\n",
      "[0.34044280648231506, 0.3605743646621704, 0.2989828586578369]\n",
      "[0.3514493703842163, 0.3346446454524994, 0.3139060139656067]\n",
      "[0.4262823164463043, 0.09120488166809082, 0.482512891292572]\n",
      "[0.4266471266746521, 0.08418235182762146, 0.48917046189308167]\n",
      "[0.4067285358905792, 0.18471191823482513, 0.40855953097343445]\n",
      "[0.06817211955785751, 0.8916477560997009, 0.040180083364248276]\n",
      "[0.3961579501628876, 0.21778038144111633, 0.3860616981983185]\n",
      "[0.4090348184108734, 0.02456059120595455, 0.5664045810699463]\n",
      "[0.4267326891422272, 0.07657566666603088, 0.49669161438941956]\n",
      "[0.39440077543258667, 0.2229471206665039, 0.3826521337032318]\n",
      "[0.4233575463294983, 0.11568301916122437, 0.46095943450927734]\n",
      "[0.42329299449920654, 0.11607856303453445, 0.4606284499168396]\n",
      "[0.38684791326522827, 0.2443895936012268, 0.3687625527381897]\n",
      "[0.13123060762882233, 0.7811330556869507, 0.08763632923364639]\n",
      "[0.40708452463150024, 0.18352197110652924, 0.4093934893608093]\n",
      "[0.41443848609924316, 0.1571875661611557, 0.42837396264076233]\n",
      "[0.3680500388145447, 0.29391053318977356, 0.33803942799568176]\n",
      "[0.42629021406173706, 0.09109058231115341, 0.4826192557811737]\n",
      "[0.4013534486293793, 0.20201002061367035, 0.3966365158557892]\n",
      "[0.3976077735424042, 0.21345792710781097, 0.38893434405326843]\n",
      "[0.42669329047203064, 0.0825948417186737, 0.4907117784023285]\n",
      "[0.4221644997596741, 0.12258739769458771, 0.4552481472492218]\n",
      "[0.4027928113937378, 0.19748933613300323, 0.39971786737442017]\n",
      "[0.42654362320899963, 0.07076495885848999, 0.5026914477348328]\n",
      "[0.42659178376197815, 0.0856565460562706, 0.48775163292884827]\n",
      "[0.40647053718566895, 0.18557049334049225, 0.4079590141773224]\n",
      "[0.40904125571250916, 0.024566881358623505, 0.5663918852806091]\n",
      "[0.33938515186309814, 0.36302828788757324, 0.29758650064468384]\n",
      "[0.3982989192008972, 0.2113770842552185, 0.3903239369392395]\n",
      "[0.10961387306451797, 0.819789469242096, 0.07059664279222488]\n",
      "[0.3158901333808899, 0.4161270260810852, 0.2679828405380249]\n",
      "[0.4111863374710083, 0.16930347681045532, 0.4195100963115692]\n",
      "[0.38737353682518005, 0.24293285608291626, 0.36969366669654846]\n",
      "[0.2414681613445282, 0.5718894600868225, 0.18664242327213287]\n",
      "[0.13655181229114532, 0.7715006470680237, 0.09194758534431458]\n",
      "[0.24645394086837769, 0.5618959665298462, 0.1916501820087433]\n",
      "[0.41854676604270935, 0.03747349977493286, 0.543979823589325]\n",
      "[0.3818058669567108, 0.2581339180469513, 0.3600601553916931]\n",
      "[0.40165457129478455, 0.20107027888298035, 0.3972751200199127]\n",
      "[0.13838566839694977, 0.7681703567504883, 0.09344395995140076]\n",
      "[0.4074103534221649, 0.1824273318052292, 0.4101622998714447]\n",
      "[0.4075077474117279, 0.18209899961948395, 0.41039320826530457]\n",
      "[0.40905410051345825, 0.02457951009273529, 0.5663663148880005]\n",
      "[0.3879788815975189, 0.24124900996685028, 0.3707720637321472]\n",
      "[0.18890824913978577, 0.6742751598358154, 0.13681660592556]\n",
      "[0.35489439964294434, 0.32636746764183044, 0.31873804330825806]\n",
      "[0.40903785824775696, 0.024563541635870934, 0.5663986206054688]\n",
      "[0.3683471083641052, 0.2931600511074066, 0.33849287033081055]\n",
      "[0.41722145676612854, 0.1459674835205078, 0.43681102991104126]\n",
      "[0.3870581090450287, 0.2438076138496399, 0.3691343367099762]\n",
      "[0.3767608404159546, 0.27151793241500854, 0.35172122716903687]\n",
      "[0.14132139086723328, 0.7628279328346252, 0.09585075080394745]\n",
      "[0.3541640341281891, 0.3281293511390686, 0.3177066445350647]\n",
      "[0.2660011053085327, 0.5221787095069885, 0.21182017028331757]\n",
      "[0.4145529866218567, 0.15674354135990143, 0.4287034571170807]\n",
      "[0.31470581889152527, 0.4187399744987488, 0.26655420660972595]\n",
      "[0.37140583992004395, 0.2853809893131256, 0.34321314096450806]\n",
      "[0.3077884912490845, 0.43389517068862915, 0.25831636786460876]\n",
      "[0.29428964853286743, 0.4629863500595093, 0.2427240014076233]\n",
      "[0.31769993901252747, 0.4121233820915222, 0.2701766788959503]\n",
      "[0.14224648475646973, 0.7611414790153503, 0.09661202877759933]\n",
      "[0.21779826283454895, 0.6186400651931763, 0.16356176137924194]\n",
      "[0.4108034670352936, 0.026383746415376663, 0.5628127455711365]\n",
      "[0.12406127154827118, 0.7940384149551392, 0.0819002315402031]\n",
      "[0.24568970501422882, 0.5634312629699707, 0.19087910652160645]\n",
      "[0.3081047832965851, 0.43320590257644653, 0.25868919491767883]\n",
      "[0.4011572599411011, 0.2026207149028778, 0.3962220549583435]\n",
      "[0.35025447607040405, 0.33749639987945557, 0.312249094247818]\n",
      "[0.3382980525493622, 0.3655443489551544, 0.2961576282978058]\n",
      "[0.31956854462623596, 0.40797579288482666, 0.272455632686615]\n",
      "[0.40903475880622864, 0.02456054650247097, 0.5664046406745911]\n",
      "[0.4090348184108734, 0.024560563266277313, 0.5664046406745911]\n",
      "[0.097537562251091, 0.841046154499054, 0.06141628324985504]\n",
      "[0.4261082410812378, 0.09352796524763107, 0.48036378622055054]\n",
      "[0.34369513392448425, 0.3529885709285736, 0.30331629514694214]\n",
      "[0.4092336893081665, 0.02475646696984768, 0.5660098195075989]\n",
      "[0.2439158707857132, 0.5669898986816406, 0.18909426033496857]\n",
      "[0.3329082429409027, 0.37792670726776123, 0.28916510939598083]\n",
      "[0.2159430831670761, 0.6222581267356873, 0.16179879009723663]\n",
      "[0.36798641085624695, 0.2940711975097656, 0.3379424214363098]\n",
      "[0.37366652488708496, 0.27956825494766235, 0.3467651903629303]\n",
      "[0.30073365569114685, 0.449174702167511, 0.2500916123390198]\n",
      "[0.3613795340061188, 0.3105480968952179, 0.32807233929634094]\n",
      "[0.4209095537662506, 0.12914608418941498, 0.4499443769454956]\n",
      "[0.4098500907421112, 0.025377871468663216, 0.5647720098495483]\n",
      "[0.3779103755950928, 0.2684977054595947, 0.3535918891429901]\n",
      "[0.34258636832237244, 0.3555816113948822, 0.3018321096897125]\n",
      "[0.41997671127319336, 0.13368919491767883, 0.4463340938091278]\n",
      "[0.2818433940410614, 0.48930251598358154, 0.22885411977767944]\n",
      "[0.4135148823261261, 0.02958887815475464, 0.5568962693214417]\n",
      "[0.2830369174480438, 0.4867984354496002, 0.23016469180583954]\n",
      "[0.25390544533729553, 0.5468589067459106, 0.19923561811447144]\n",
      "[0.23723165690898895, 0.5803398489952087, 0.1824285238981247]\n",
      "[0.20801100134849548, 0.6376556754112244, 0.1543332189321518]\n",
      "[0.2728066146373749, 0.5081361532211304, 0.2190573364496231]\n",
      "[0.36201009154319763, 0.3089921772480011, 0.32899779081344604]\n",
      "[0.42626118659973145, 0.09150544553995132, 0.4822334051132202]\n",
      "[0.35665494203567505, 0.3221048414707184, 0.3212401866912842]\n",
      "[0.3850861191749573, 0.24923793971538544, 0.3656758964061737]\n",
      "[0.3459126353263855, 0.34778091311454773, 0.306306391954422]\n",
      "[0.4188408851623535, 0.038047224283218384, 0.5431119203567505]\n",
      "[0.3302055895328522, 0.3840806186199188, 0.2857137620449066]\n",
      "[0.35976120829582214, 0.31452661752700806, 0.3257121741771698]\n",
      "[0.2449677586555481, 0.56488037109375, 0.1901518702507019]\n",
      "[0.029466338455677032, 0.9554964303970337, 0.01503724418580532]\n",
      "[0.3280300199985504, 0.38900917768478394, 0.28296080231666565]\n",
      "[0.33878156542778015, 0.3644261062145233, 0.29679232835769653]\n",
      "[0.42506512999534607, 0.10375555604696274, 0.4711792767047882]\n",
      "[0.4095880091190338, 0.02511095628142357, 0.5653010606765747]\n",
      "[0.10377901792526245, 0.8300910592079163, 0.06612999737262726]\n",
      "[0.3914814293384552, 0.23137246072292328, 0.3771461248397827]\n",
      "[0.4266999661922455, 0.08231714367866516, 0.49098291993141174]\n",
      "[0.13513235747814178, 0.7740745544433594, 0.09079308807849884]\n",
      "[0.4266526401042938, 0.0732128918170929, 0.5001344680786133]\n",
      "[0.37181028723716736, 0.28434523940086365, 0.3438445031642914]\n",
      "[0.14340093731880188, 0.759035050868988, 0.09756399691104889]\n",
      "[0.3450440764427185, 0.3498241901397705, 0.3051316738128662]\n",
      "[0.3033320903778076, 0.4435670077800751, 0.2531009912490845]\n",
      "[0.42182907462120056, 0.045049719512462616, 0.5331212282180786]\n",
      "[0.38302555680274963, 0.2548453211784363, 0.3621291518211365]\n",
      "[0.4090348780155182, 0.024560634046792984, 0.5664045214653015]\n",
      "[0.17411433160305023, 0.7022035717964172, 0.12368206679821014]\n",
      "[0.412526398897171, 0.16441814601421356, 0.42305538058280945]\n",
      "[0.38494592905044556, 0.2496216595172882, 0.3654324412345886]\n",
      "[0.4090683162212372, 0.024593453854322433, 0.5663381814956665]\n",
      "[0.3303132951259613, 0.3838360607624054, 0.2858506143093109]\n",
      "[0.357678085565567, 0.31961697340011597, 0.322704941034317]\n",
      "[0.3946457803249359, 0.2222311943769455, 0.3831230401992798]\n",
      "[0.41854700446128845, 0.03747405484318733, 0.5439789295196533]\n",
      "[0.32769498229026794, 0.38976621627807617, 0.2825388014316559]\n",
      "[0.40114253759384155, 0.2026664763689041, 0.3961910307407379]\n",
      "[0.35728272795677185, 0.32057932019233704, 0.3221379816532135]\n",
      "[0.31209176778793335, 0.42448827624320984, 0.26342007517814636]\n",
      "[0.15237052738666534, 0.7425960898399353, 0.10503347218036652]\n",
      "[0.388872891664505, 0.23875032365322113, 0.37237679958343506]\n",
      "[0.15737268328666687, 0.7333718538284302, 0.10925544053316116]\n",
      "[0.3649462163448334, 0.3017018139362335, 0.3333519399166107]\n",
      "[0.34419703483581543, 0.3518124222755432, 0.3039904832839966]\n",
      "[0.2105238139629364, 0.6327904462814331, 0.1566857397556305]\n",
      "[0.15674683451652527, 0.7345281839370728, 0.10872500389814377]\n",
      "[0.33758121728897095, 0.3671998679637909, 0.29521891474723816]\n",
      "[0.13168665766716003, 0.7803092002868652, 0.08800404518842697]\n",
      "[0.4093886911869049, 0.1756587028503418, 0.4149526357650757]\n",
      "[0.2321901023387909, 0.5903478264808655, 0.17746207118034363]\n",
      "[0.41496437788009644, 0.15513692796230316, 0.42989858984947205]\n",
      "[0.41007208824157715, 0.025607075542211533, 0.5643208622932434]\n",
      "[0.1211278960108757, 0.799294650554657, 0.07957738637924194]\n",
      "[0.3393802344799042, 0.3630397319793701, 0.2975800037384033]\n",
      "[0.1627870351076126, 0.7233419418334961, 0.1138710156083107]\n",
      "[0.36719590425491333, 0.296063631772995, 0.3367404341697693]\n",
      "[0.12409472465515137, 0.7939784526824951, 0.08192679286003113]\n",
      "[0.2734457850456238, 0.5068110823631287, 0.21974313259124756]\n",
      "[0.4009819030761719, 0.20316530764102936, 0.3958527743816376]\n",
      "[0.3920495808124542, 0.22974732518196106, 0.3782030940055847]\n",
      "[0.29142114520072937, 0.4690922498703003, 0.23948650062084198]\n",
      "[0.38468024134635925, 0.2503478229045868, 0.36497196555137634]\n",
      "[0.22862258553504944, 0.5973986983299255, 0.17397865653038025]\n",
      "[0.31473642587661743, 0.41867247223854065, 0.26659107208251953]\n",
      "[0.28584980964660645, 0.4808809161186218, 0.2332693338394165]\n",
      "[0.37948548793792725, 0.2643319368362427, 0.35618260502815247]\n",
      "[0.10282058268785477, 0.8317775726318359, 0.06540181487798691]\n",
      "[0.41118043661117554, 0.1693248301744461, 0.41949474811553955]\n",
      "[0.3624693751335144, 0.30785664916038513, 0.32967397570610046]\n",
      "[0.4241673946380615, 0.11043732613325119, 0.4653952419757843]\n",
      "[0.4090375304222107, 0.024563239887356758, 0.5663992166519165]\n",
      "[0.379989355802536, 0.26299238204956055, 0.35701826214790344]\n",
      "[0.4134073257446289, 0.16112804412841797, 0.42546460032463074]\n",
      "[0.322466105222702, 0.4015157222747803, 0.2760181128978729]\n",
      "[0.40104174613952637, 0.2029794156551361, 0.39597874879837036]\n",
      "[0.15842190384864807, 0.7314320206642151, 0.11014614999294281]\n",
      "[0.42673999071121216, 0.07970429956912994, 0.4935557544231415]\n",
      "[0.10046941041946411, 0.8359083533287048, 0.0636221393942833]\n",
      "[0.3924699127674103, 0.22854052484035492, 0.3789895176887512]\n",
      "[0.41185104846954346, 0.1668975055217743, 0.42125147581100464]\n",
      "[0.3742268979549408, 0.27811872959136963, 0.3476543724536896]\n",
      "[0.42348337173461914, 0.11490356177091599, 0.4616130590438843]\n",
      "[0.2313932627439499, 0.5919249057769775, 0.17668184638023376]\n",
      "[0.40806594491004944, 0.1802082508802414, 0.41172584891319275]\n",
      "[0.18119007349014282, 0.6888916492462158, 0.12991827726364136]\n",
      "[0.4223179519176483, 0.12174128741025925, 0.4559406638145447]\n",
      "[0.36602702736854553, 0.2989989221096039, 0.3349740505218506]\n",
      "[0.19307652115821838, 0.6663391590118408, 0.14058434963226318]\n",
      "[0.39999035000801086, 0.20622621476650238, 0.39378345012664795]\n",
      "[0.41078782081604004, 0.17073093354701996, 0.4184812307357788]\n",
      "[0.3946169316768646, 0.22231556475162506, 0.3830675184726715]\n",
      "[0.17222225666046143, 0.7057490944862366, 0.12202859669923782]\n",
      "[0.416551798582077, 0.14875692129135132, 0.43469128012657166]\n",
      "[0.34503453969955444, 0.3498465418815613, 0.3051188588142395]\n",
      "[0.3990632891654968, 0.20906004309654236, 0.3918766975402832]\n",
      "[0.37612220644950867, 0.2731887698173523, 0.35068902373313904]\n",
      "[0.4254642426967621, 0.10030823200941086, 0.4742274880409241]\n",
      "[0.3287220895290375, 0.38744375109672546, 0.28383415937423706]\n",
      "[0.3847143054008484, 0.2502547800540924, 0.3650309443473816]\n",
      "[0.4247572124004364, 0.10618387162685394, 0.46905896067619324]\n",
      "[0.40929776430130005, 0.17597459256649017, 0.4147276282310486]\n",
      "[0.35697025060653687, 0.3213390111923218, 0.32169073820114136]\n",
      "[0.20096610486507416, 0.6512355208396912, 0.14779840409755707]\n",
      "[0.37348100543022156, 0.2800474166870117, 0.34647154808044434]\n",
      "[0.21779823303222656, 0.618640124797821, 0.16356170177459717]\n",
      "[0.37187907099723816, 0.2841688394546509, 0.34395208954811096]\n",
      "[0.33413034677505493, 0.3751320242881775, 0.2907375991344452]\n",
      "[0.27705469727516174, 0.49930983781814575, 0.22363540530204773]\n",
      "[0.21815083920955658, 0.6179516315460205, 0.1638975292444229]\n",
      "[0.21503259241580963, 0.6240314841270447, 0.16093595325946808]\n",
      "[0.4105159640312195, 0.026074402034282684, 0.5634096264839172]\n",
      "[0.40904849767684937, 0.02457399107515812, 0.5663775205612183]\n",
      "[0.42672285437583923, 0.07590170204639435, 0.4973754584789276]\n",
      "[0.31209173798561096, 0.42448827624320984, 0.263420045375824]\n",
      "[0.2818252742290497, 0.4893404543399811, 0.22883424162864685]\n",
      "[0.42487117648124695, 0.10530456155538559, 0.4698242247104645]\n",
      "[0.29407256841659546, 0.4634493887424469, 0.24247808754444122]\n",
      "[0.24382559955120087, 0.5671707391738892, 0.18900364637374878]\n",
      "[0.42641469836235046, 0.08915343880653381, 0.4844318628311157]\n",
      "[0.16026315093040466, 0.7280231714248657, 0.11171358078718185]\n",
      "[0.34985196590423584, 0.3384549617767334, 0.31169310212135315]\n",
      "[0.2707025706768036, 0.5124901533126831, 0.2168073207139969]\n",
      "[0.30675020813941956, 0.436154842376709, 0.25709497928619385]\n",
      "[0.3794834315776825, 0.2643373906612396, 0.3561791777610779]\n",
      "[0.4258127510547638, 0.0968959704041481, 0.4772912859916687]\n",
      "[0.36291947960853577, 0.3067421317100525, 0.33033838868141174]\n",
      "[0.322658896446228, 0.40108469128608704, 0.27625638246536255]\n",
      "[0.37191998958587646, 0.2840639054775238, 0.34401607513427734]\n",
      "[0.4110422730445862, 0.16982074081897736, 0.4191369116306305]\n",
      "[0.15765303373336792, 0.7328536510467529, 0.10949329286813736]\n",
      "[0.3363491892814636, 0.37003886699676514, 0.29361191391944885]\n",
      "[0.04583742469549179, 0.9289597868919373, 0.025202807039022446]\n",
      "[0.3935045599937439, 0.22555378079414368, 0.38094162940979004]\n",
      "[0.3969596326351166, 0.2153971791267395, 0.3876432180404663]\n",
      "[0.32350701093673706, 0.3991864323616028, 0.27730655670166016]\n",
      "[0.3918485641479492, 0.23032306134700775, 0.3778283894062042]\n",
      "[0.3606462776660919, 0.31235337257385254, 0.32700031995773315]\n",
      "[0.2992384731769562, 0.4523914158344269, 0.24837015569210052]\n",
      "[0.42174437642097473, 0.124849833548069, 0.45340582728385925]\n",
      "[0.3722900450229645, 0.2831142544746399, 0.3445957601070404]\n",
      "[0.37729412317276, 0.2701188623905182, 0.3525869846343994]\n",
      "[0.409249484539032, 0.17614230513572693, 0.41460826992988586]\n",
      "[0.42662593722343445, 0.08478444069623947, 0.4885895550251007]\n",
      "[0.32656630873680115, 0.39231276512145996, 0.2811209559440613]\n",
      "[0.3831821382045746, 0.25442156195640564, 0.36239635944366455]\n",
      "[0.1552465558052063, 0.7372974753379822, 0.10745599120855331]\n",
      "[0.4220389723777771, 0.12327118217945099, 0.4546898603439331]\n",
      "[0.4224299490451813, 0.12111734598875046, 0.45645272731781006]\n",
      "[0.26001596450805664, 0.5344341397285461, 0.20554988086223602]\n",
      "[0.40928560495376587, 0.024807943031191826, 0.5659064650535583]\n",
      "[0.20301629602909088, 0.6472926735877991, 0.14969100058078766]\n",
      "[0.4093777537345886, 0.024899732321500778, 0.5657225251197815]\n",
      "[0.42142829298973083, 0.12650562822818756, 0.4520661532878876]\n",
      "[0.3927232623100281, 0.22781148552894592, 0.3794652819633484]\n",
      "[0.281698077917099, 0.48960715532302856, 0.22869479656219482]\n",
      "[0.2820836007595062, 0.4887988269329071, 0.22911754250526428]\n",
      "[0.40038809180259705, 0.2050020843744278, 0.3946097493171692]\n",
      "[0.06783566623926163, 0.8922180533409119, 0.03994634002447128]\n",
      "[0.1373949944972992, 0.7699700593948364, 0.09263493120670319]\n",
      "[0.3095701038837433, 0.43000873923301697, 0.26042112708091736]\n",
      "[0.3801518976688385, 0.2625595033168793, 0.3572885990142822]\n",
      "[0.1601773351430893, 0.7281822562217712, 0.11164038628339767]\n",
      "[0.40903547406196594, 0.024561237543821335, 0.5664032697677612]\n",
      "[0.27280735969543457, 0.5081344842910767, 0.21905817091464996]\n",
      "[0.3300849497318268, 0.38435450196266174, 0.2855605185031891]\n",
      "[0.4240304231643677, 0.11136641353368759, 0.46460315585136414]\n",
      "[0.4021534025669098, 0.19950655102729797, 0.39833998680114746]\n",
      "[0.3360078036785126, 0.3708242177963257, 0.29316797852516174]\n",
      "[0.22750955820083618, 0.5995935201644897, 0.17289701104164124]\n",
      "[0.42415913939476013, 0.11049411445856094, 0.4653467833995819]\n",
      "[0.410068154335022, 0.02560299262404442, 0.5643288493156433]\n",
      "[0.18161161243915558, 0.6880958676338196, 0.13029244542121887]\n",
      "[0.41104230284690857, 0.16982081532478333, 0.4191369116306305]\n",
      "[0.2439158707857132, 0.5669898986816406, 0.18909426033496857]\n",
      "[0.21421699225902557, 0.6256186366081238, 0.16016434133052826]\n",
      "[0.36884939670562744, 0.29188916087150574, 0.3392615020275116]\n",
      "[0.3161928951740265, 0.41545817255973816, 0.26834893226623535]\n",
      "[0.4136222004890442, 0.1603151559829712, 0.42606261372566223]\n",
      "[0.4114779233932495, 0.1682521551847458, 0.4202699363231659]\n",
      "[0.37655970454216003, 0.27204465866088867, 0.3513956069946289]\n",
      "[0.4170568883419037, 0.034795425832271576, 0.5481476783752441]\n",
      "[0.304663747549057, 0.440684050321579, 0.254652202129364]\n",
      "[0.3490815758705139, 0.34028658270835876, 0.3106318414211273]\n",
      "[0.1184067502617836, 0.804158091545105, 0.07743518054485321]\n",
      "[0.38402223587036133, 0.2521413564682007, 0.363836407661438]\n",
      "[0.4257972538471222, 0.09705834090709686, 0.4771444499492645]\n",
      "[0.2981909513473511, 0.4546406865119934, 0.2471684366464615]\n",
      "[0.3798428177833557, 0.26338228583335876, 0.35677486658096313]\n",
      "[0.30727332830429077, 0.4350167512893677, 0.25770992040634155]\n",
      "[0.36070358753204346, 0.31221237778663635, 0.3270839750766754]\n",
      "[0.29788288474082947, 0.4553014039993286, 0.2468157261610031]\n",
      "[0.4037895202636719, 0.19431409239768982, 0.4018963575363159]\n",
      "[0.36798301339149475, 0.29407966136932373, 0.33793726563453674]\n",
      "[0.23424610495567322, 0.586272656917572, 0.17948122322559357]\n",
      "[0.4163150191307068, 0.1497284471988678, 0.4339565336704254]\n",
      "[0.4200933277606964, 0.13313421607017517, 0.4467724859714508]\n",
      "[0.18873386085033417, 0.6746065020561218, 0.136659637093544]\n",
      "[0.40805596113204956, 0.18024224042892456, 0.41170185804367065]\n",
      "[0.38524332642555237, 0.2488074004650116, 0.36594924330711365]\n",
      "[0.28763577342033386, 0.47711190581321716, 0.23525235056877136]\n",
      "[0.30543801188468933, 0.4390051066875458, 0.2555569112300873]\n",
      "[0.29409852623939514, 0.4633939266204834, 0.24250751733779907]\n",
      "[0.22027520835399628, 0.6137992143630981, 0.16592560708522797]\n",
      "[0.4090348482131958, 0.024560611695051193, 0.5664045214653015]\n",
      "[0.34921079874038696, 0.33997952938079834, 0.3108096122741699]\n",
      "[0.42520248889923096, 0.05829924717545509, 0.5164982676506042]\n",
      "[0.4088533818721771, 0.17751194536685944, 0.413634717464447]\n",
      "[0.33579403162002563, 0.37131550908088684, 0.29289039969444275]\n",
      "[0.3467000126838684, 0.3459246754646301, 0.3073752820491791]\n",
      "[0.40357398986816406, 0.19500401616096497, 0.40142199397087097]\n",
      "[0.23363842070102692, 0.5874781012535095, 0.17888352274894714]\n",
      "[0.4102413058280945, 0.17267078161239624, 0.4170879125595093]\n",
      "[0.4161231219768524, 0.15051040053367615, 0.4333665072917938]\n",
      "[0.42195719480514526, 0.12371303886175156, 0.45432978868484497]\n",
      "[0.4105279743671417, 0.17165568470954895, 0.41781628131866455]\n",
      "[0.4102751314640045, 0.025819357484579086, 0.5639054775238037]\n",
      "[0.05208670347929001, 0.9186358451843262, 0.02927737683057785]\n",
      "[0.36201006174087524, 0.3089921772480011, 0.32899773120880127]\n",
      "[0.2520316243171692, 0.5506519079208374, 0.1973164677619934]\n",
      "[0.41578274965286255, 0.03275959938764572, 0.5514576435089111]\n",
      "[0.32119879126548767, 0.4043456017971039, 0.27445560693740845]\n",
      "[0.397508442401886, 0.21375583112239838, 0.38873571157455444]\n",
      "[0.3875710070133209, 0.24238421022891998, 0.3700447976589203]\n",
      "[0.16660062968730927, 0.7162486910820007, 0.11715066432952881]\n",
      "[0.4090813398361206, 0.024606196209788322, 0.5663124918937683]\n",
      "[0.2894706130027771, 0.4732300341129303, 0.23729939758777618]\n",
      "[0.33964791893959045, 0.3624193072319031, 0.29793283343315125]\n",
      "[0.32504066824913025, 0.39574572443962097, 0.2792135775089264]\n",
      "[0.3266991674900055, 0.3920132517814636, 0.2812875211238861]\n",
      "[0.3774036765098572, 0.2698310315608978, 0.3527652621269226]\n",
      "[0.3694365322589874, 0.29040026664733887, 0.3401632010936737]\n",
      "[0.3929133713245392, 0.22726333141326904, 0.3798232972621918]\n",
      "[0.4127572178840637, 0.028636468574404716, 0.5586063265800476]\n",
      "[0.22486531734466553, 0.6047975420951843, 0.17033715546131134]\n",
      "[0.3495326340198517, 0.339214563369751, 0.31125274300575256]\n",
      "[0.3449217677116394, 0.35011157393455505, 0.30496665835380554]\n",
      "[0.2817097306251526, 0.4895825982093811, 0.22870762646198273]\n",
      "[0.42229923605918884, 0.12184550613164902, 0.4558553397655487]\n",
      "[0.3933713436126709, 0.22593970596790314, 0.38068893551826477]\n",
      "[0.3226589262485504, 0.40108466148376465, 0.27625641226768494]\n",
      "[0.4090445041656494, 0.024570079520344734, 0.5663853883743286]\n",
      "[0.40909186005592346, 0.024616558104753494, 0.5662915706634521]\n",
      "[0.2450004369020462, 0.5648148059844971, 0.19018477201461792]\n",
      "[0.3534450829029083, 0.32985982298851013, 0.31669509410858154]\n",
      "[0.28859367966651917, 0.475086510181427, 0.23631979525089264]\n",
      "[0.4044598937034607, 0.19215674698352814, 0.40338343381881714]\n",
      "[0.36016637086868286, 0.31353259086608887, 0.32630106806755066]\n",
      "[0.3690318465232849, 0.2914268672466278, 0.33954134583473206]\n",
      "[0.383335679769516, 0.2540056109428406, 0.3626587390899658]\n",
      "[0.36211085319519043, 0.30874311923980713, 0.32914596796035767]\n",
      "[0.33697500824928284, 0.36859777569770813, 0.29442718625068665]\n",
      "[0.3613795042037964, 0.31054821610450745, 0.32807230949401855]\n",
      "[0.36312374472618103, 0.30623576045036316, 0.3306404948234558]\n",
      "[0.4008798599243164, 0.20348188281059265, 0.3956383168697357]\n",
      "[0.4119023382663727, 0.1667105108499527, 0.4213871955871582]\n",
      "[0.36646750569343567, 0.29789432883262634, 0.335638165473938]\n",
      "[0.41286787390708923, 0.16315068304538727, 0.4239814281463623]\n",
      "[0.12958867847919464, 0.7840960621833801, 0.08631528168916702]\n",
      "[0.2580489218235016, 0.5384433269500732, 0.20350778102874756]\n",
      "[0.3964523375034332, 0.21690724790096283, 0.3866404891014099]\n",
      "[0.4181281626224518, 0.03668433055281639, 0.5451874732971191]\n",
      "[0.33955836296081543, 0.3626268804073334, 0.2978147566318512]\n",
      "[0.3512137234210968, 0.33520767092704773, 0.3135785162448883]\n",
      "[0.3423812985420227, 0.35606035590171814, 0.30155840516090393]\n",
      "[0.23449945449829102, 0.5857698917388916, 0.179730623960495]\n",
      "[0.414753794670105, 0.1559617966413498, 0.4292844533920288]\n",
      "[0.41186052560806274, 0.027568360790610313, 0.5605711340904236]\n",
      "[0.2269202172756195, 0.6007544994354248, 0.17232529819011688]\n",
      "[0.2804115116596222, 0.49230146408081055, 0.22728705406188965]\n",
      "[0.15278233587741852, 0.7418381571769714, 0.10537951439619064]\n",
      "[0.14277158677577972, 0.7601836919784546, 0.09704473614692688]\n",
      "[0.30699971318244934, 0.43561217188835144, 0.2573881447315216]\n",
      "[0.21046225726604462, 0.6329097747802734, 0.15662798285484314]\n",
      "[0.41099587082862854, 0.16998723149299622, 0.41901692748069763]\n",
      "[0.4255445897579193, 0.06048256531357765, 0.5139728784561157]\n",
      "[0.4065507650375366, 0.18530386686325073, 0.4081454277038574]\n",
      "[0.4265104830265045, 0.08741721510887146, 0.48607227206230164]\n",
      "[0.4255051910877228, 0.09993033856153488, 0.47456449270248413]\n",
      "[0.26783326268196106, 0.5184096693992615, 0.21375708281993866]\n",
      "[0.4222886562347412, 0.12190385162830353, 0.45580747723579407]\n",
      "[0.41068175435066223, 0.026252152398228645, 0.5630660653114319]\n",
      "[0.4097086191177368, 0.02523326314985752, 0.565058171749115]\n",
      "[0.412952721118927, 0.1628343164920807, 0.4242129623889923]\n",
      "[0.41196197271347046, 0.1664927750825882, 0.4215452969074249]\n",
      "[0.24543504416942596, 0.563942551612854, 0.1906224638223648]\n",
      "[0.1365247368812561, 0.7715498208999634, 0.09192554652690887]\n",
      "[0.4197383522987366, 0.1348133087158203, 0.4454484283924103]\n",
      "[0.14668485522270203, 0.7530315518379211, 0.1002836525440216]\n",
      "[0.2961282432079315, 0.4590591490268707, 0.24481256306171417]\n",
      "[0.4138181209564209, 0.02998385950922966, 0.5561980605125427]\n",
      "[0.4184940457344055, 0.14047545194625854, 0.4410305321216583]\n",
      "[0.40412014722824097, 0.19325225055217743, 0.402627557516098]\n",
      "[0.34181153774261475, 0.3573892116546631, 0.3007992208003998]\n",
      "[0.4117504358291626, 0.027441365644335747, 0.5608081817626953]\n",
      "[0.16052737832069397, 0.727533757686615, 0.11193893104791641]\n",
      "[0.3915309011936188, 0.2312311977148056, 0.3772379159927368]\n",
      "[0.3083724081516266, 0.43262267112731934, 0.2590049207210541]\n",
      "[0.33520811796188354, 0.3726613223552704, 0.29213058948516846]\n",
      "[0.4153244197368622, 0.15371620655059814, 0.4309593737125397]\n",
      "[0.4090365469455719, 0.024562297388911247, 0.5664011240005493]\n",
      "[0.3966999650001526, 0.21617090702056885, 0.38712912797927856]\n",
      "[0.3314351439476013, 0.38128530979156494, 0.28727951645851135]\n",
      "[0.3452241122722626, 0.3494010865688324, 0.30537480115890503]\n",
      "[0.3444622755050659, 0.3511902689933777, 0.304347425699234]\n",
      "[0.18905185163021088, 0.6740022897720337, 0.136945903301239]\n",
      "[0.346401572227478, 0.34662866592407227, 0.30696970224380493]\n",
      "[0.36937472224235535, 0.2905571460723877, 0.34006810188293457]\n",
      "[0.4104353189468384, 0.17198452353477478, 0.41758018732070923]\n",
      "[0.4161362648010254, 0.033303797245025635, 0.550559937953949]\n",
      "[0.4259704351425171, 0.09516921639442444, 0.4788603186607361]\n",
      "[0.40911954641342163, 0.024643760174512863, 0.5662367343902588]\n",
      "[0.27134984731674194, 0.5111518502235413, 0.217498317360878]\n",
      "[0.24313616752624512, 0.5685519576072693, 0.1883118599653244]\n",
      "[0.394423246383667, 0.2228815257549286, 0.3826952576637268]\n",
      "[0.39807412028312683, 0.21205542981624603, 0.38987046480178833]\n",
      "[0.4071642756462097, 0.18325455486774445, 0.40958115458488464]\n",
      "[0.14150118827819824, 0.7625002264976501, 0.09599857777357101]\n",
      "[0.4091232419013977, 0.02464739978313446, 0.5662294030189514]\n",
      "[0.4231395423412323, 0.11700735241174698, 0.4598531424999237]\n",
      "[0.4239049255847931, 0.051999565213918686, 0.5240955352783203]\n",
      "[0.06142681837081909, 0.9030339121818542, 0.03553927317261696]\n",
      "[0.4099518954753876, 0.025482598692178726, 0.5645655393600464]\n",
      "[0.40708452463150024, 0.18352197110652924, 0.4093934893608093]\n",
      "[0.4090631902217865, 0.02458839677274227, 0.5663483738899231]\n",
      "[0.28859367966651917, 0.475086510181427, 0.23631979525089264]\n",
      "[0.4263675808906555, 0.08992008864879608, 0.4837122857570648]\n",
      "[0.3973495364189148, 0.21423190832138062, 0.3884185254573822]\n",
      "[0.15505152940750122, 0.7376571893692017, 0.10729128867387772]\n",
      "[0.3077601194381714, 0.4339568316936493, 0.25828295946121216]\n",
      "[0.40293630957603455, 0.19703443348407745, 0.4000292122364044]\n",
      "[0.2948623299598694, 0.4617643654346466, 0.24337337911128998]\n",
      "[0.2360365241765976, 0.5827170014381409, 0.18124651908874512]\n",
      "[0.30880311131477356, 0.431683212518692, 0.25951364636421204]\n",
      "[0.2696439027786255, 0.5146766304969788, 0.21567946672439575]\n",
      "[0.41578271985054016, 0.03275958448648453, 0.5514576435089111]\n",
      "[0.30597391724586487, 0.4378417730331421, 0.25618433952331543]\n",
      "[0.4092012643814087, 0.02472439594566822, 0.5660743117332458]\n",
      "[0.42663490772247314, 0.08453630656003952, 0.48882871866226196]\n",
      "[0.4211273789405823, 0.12804822623729706, 0.45082440972328186]\n",
      "[0.4237509071826935, 0.11320601403713226, 0.46304309368133545]\n",
      "[0.21314235031604767, 0.6277081370353699, 0.15914954245090485]\n",
      "[0.11813382804393768, 0.8046451210975647, 0.07722101360559464]\n",
      "[0.328676700592041, 0.387546569108963, 0.28377678990364075]\n",
      "[0.42674025893211365, 0.0796704962849617, 0.49358928203582764]\n",
      "[0.42398902773857117, 0.11164315789937973, 0.46436771750450134]\n",
      "[0.12554088234901428, 0.7913818955421448, 0.08307721465826035]\n",
      "[0.3320107161998749, 0.37997424602508545, 0.2880150377750397]\n",
      "[0.3867676258087158, 0.244611456990242, 0.3686208426952362]\n",
      "[0.24348756670951843, 0.5678480863571167, 0.18866434693336487]\n",
      "[0.41703444719314575, 0.1467529833316803, 0.43621253967285156]\n",
      "[0.29925185441970825, 0.45236268639564514, 0.248385488986969]\n",
      "[0.35513007640838623, 0.32579824328422546, 0.3190716505050659]\n",
      "[0.40668797492980957, 0.1848471462726593, 0.4084649085998535]\n",
      "[0.41002342104911804, 0.17343874275684357, 0.4165378510951996]\n",
      "[0.29738858342170715, 0.4563610553741455, 0.24625042080879211]\n",
      "[0.34385961294174194, 0.35260331630706787, 0.3035370707511902]\n",
      "[0.37348470091819763, 0.2800379693508148, 0.34647735953330994]\n",
      "[0.3589409291744232, 0.3165351152420044, 0.32452392578125]\n",
      "[0.2810268998146057, 0.4910132586956024, 0.22795984148979187]\n",
      "[0.39627310633659363, 0.21743905544281006, 0.3862878382205963]\n",
      "[0.3937358558177948, 0.2248828262090683, 0.3813813030719757]\n",
      "[0.40913835167884827, 0.024662280455231667, 0.5661993622779846]\n",
      "[0.4243977963924408, 0.108828604221344, 0.4667735993862152]\n",
      "[0.22970952093601227, 0.5952531695365906, 0.17503730952739716]\n",
      "[0.3246399462223053, 0.396645724773407, 0.2787143290042877]\n",
      "[0.4068866968154907, 0.1841839849948883, 0.4089292883872986]\n",
      "[0.36162304878234863, 0.309947669506073, 0.32842934131622314]\n",
      "[0.34333521127700806, 0.35383111238479614, 0.3028337061405182]\n",
      "[0.2528156340122223, 0.549065887928009, 0.19811849296092987]\n",
      "[0.40235498547554016, 0.19887232780456543, 0.3987727463245392]\n",
      "[0.3652085065841675, 0.30104687809944153, 0.3337445855140686]\n",
      "[0.4266452491283417, 0.08423931151628494, 0.48911547660827637]\n",
      "[0.396812379360199, 0.21583619713783264, 0.38735145330429077]\n",
      "[0.07317063957452774, 0.8831499814987183, 0.0436793752014637]\n",
      "[0.37409016489982605, 0.2784728705883026, 0.3474370539188385]\n",
      "[0.3324722349643707, 0.378921777009964, 0.2886059284210205]\n",
      "[0.42643266916275024, 0.08884771168231964, 0.4847196042537689]\n",
      "[0.03505725786089897, 0.9465247988700867, 0.018417945131659508]\n",
      "[0.2097667157649994, 0.6342576146125793, 0.15597571432590485]\n",
      "[0.364919513463974, 0.30176836252212524, 0.3333120346069336]\n",
      "[0.4189656972885132, 0.13836669921875, 0.44266751408576965]\n",
      "[0.3324931561946869, 0.3788739740848541, 0.28863275051116943]\n",
      "[0.23866227269172668, 0.5774903893470764, 0.1838473379611969]\n",
      "[0.40693172812461853, 0.18403348326683044, 0.409034788608551]\n",
      "[0.2888627052307129, 0.47451719641685486, 0.23662008345127106]\n",
      "[0.42525923252105713, 0.05863960459828377, 0.5161012411117554]\n",
      "[0.40903475880622864, 0.02456054650247097, 0.5664046406745911]\n",
      "[0.40499788522720337, 0.190411776304245, 0.404590368270874]\n",
      "[0.4264782965183258, 0.08803292363882065, 0.4854888319969177]\n",
      "[0.4038725793361664, 0.194047749042511, 0.40207961201667786]\n",
      "[0.37541860342025757, 0.27502384781837463, 0.3495575189590454]\n",
      "[0.3345921039581299, 0.37407419085502625, 0.2913336753845215]\n",
      "[0.408121794462204, 0.18001802265644073, 0.4118601679801941]\n",
      "[0.41275930404663086, 0.0286390483379364, 0.5586016178131104]\n",
      "[0.40909725427627563, 0.02462179958820343, 0.5662810206413269]\n",
      "[0.26958149671554565, 0.5148055553436279, 0.21561305224895477]\n",
      "[0.28448137640953064, 0.4837625026702881, 0.23175616562366486]\n",
      "[0.3947746455669403, 0.22185412049293518, 0.3833712637424469]\n",
      "[0.4123908579349518, 0.028192691504955292, 0.5594164133071899]\n",
      "[0.12972627580165863, 0.783847987651825, 0.08642580360174179]\n",
      "[0.3981587588787079, 0.21180005371570587, 0.3900410830974579]\n",
      "[0.42470619082450867, 0.10657066106796265, 0.46872320771217346]\n",
      "[0.19921642541885376, 0.6545944809913635, 0.1461891233921051]\n",
      "[0.1511581540107727, 0.744825541973114, 0.10401629656553268]\n",
      "[0.426596999168396, 0.08553063869476318, 0.4878724217414856]\n",
      "[0.15460285544395447, 0.7384845018386841, 0.10691263526678085]\n",
      "[0.2368556410074234, 0.5810880064964294, 0.18205632269382477]\n",
      "[0.06895767152309418, 0.8903157114982605, 0.040726691484451294]\n",
      "[0.4023148715496063, 0.19899852573871613, 0.39868655800819397]\n",
      "[0.41008079051971436, 0.025616124272346497, 0.5643031001091003]\n",
      "[0.40953177213668823, 0.02505418471992016, 0.5654141306877136]\n",
      "[0.2682484984397888, 0.5175543427467346, 0.21419717371463776]\n",
      "[0.410619854927063, 0.17132937908172607, 0.4180507957935333]\n",
      "[0.1888975203037262, 0.6742955446243286, 0.13680697977542877]\n",
      "[0.16595666110515594, 0.7174481749534607, 0.11659520119428635]\n",
      "[0.41385847330093384, 0.030037058517336845, 0.5561044812202454]\n",
      "[0.04134189710021019, 0.9363242983818054, 0.022333776578307152]\n",
      "[0.41807955503463745, 0.14229416847229004, 0.4396262466907501]\n",
      "[0.41418448090553284, 0.03047221153974533, 0.5553432703018188]\n",
      "[0.25657379627227783, 0.5414438247680664, 0.201982319355011]\n",
      "[0.3725362718105316, 0.2824813723564148, 0.3449822962284088]\n",
      "[0.3026899993419647, 0.4449547827243805, 0.2523552477359772]\n",
      "[0.4265263080596924, 0.08710061013698578, 0.48637309670448303]\n"
     ]
    }
   ],
   "source": [
    "for i in pred:\n",
    "    print(i['meta'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inp': [[2,\n",
       "   34,\n",
       "   3,\n",
       "   7,\n",
       "   27,\n",
       "   8,\n",
       "   21,\n",
       "   19,\n",
       "   8,\n",
       "   5,\n",
       "   10,\n",
       "   21,\n",
       "   23,\n",
       "   5,\n",
       "   14,\n",
       "   8,\n",
       "   21,\n",
       "   19,\n",
       "   8,\n",
       "   16,\n",
       "   22,\n",
       "   31,\n",
       "   38,\n",
       "   8,\n",
       "   16,\n",
       "   19,\n",
       "   24,\n",
       "   5,\n",
       "   10,\n",
       "   19,\n",
       "   8,\n",
       "   3,\n",
       "   5,\n",
       "   31,\n",
       "   9,\n",
       "   8,\n",
       "   7,\n",
       "   7,\n",
       "   34,\n",
       "   13,\n",
       "   5,\n",
       "   3,\n",
       "   8,\n",
       "   5,\n",
       "   7,\n",
       "   19,\n",
       "   16,\n",
       "   10,\n",
       "   19,\n",
       "   18,\n",
       "   8,\n",
       "   21,\n",
       "   19,\n",
       "   8,\n",
       "   9,\n",
       "   24],\n",
       "  [2,\n",
       "   23,\n",
       "   5,\n",
       "   4,\n",
       "   3,\n",
       "   10,\n",
       "   8,\n",
       "   31,\n",
       "   19,\n",
       "   31,\n",
       "   14,\n",
       "   8,\n",
       "   19,\n",
       "   38,\n",
       "   22,\n",
       "   31,\n",
       "   8,\n",
       "   9,\n",
       "   5,\n",
       "   14,\n",
       "   20,\n",
       "   18,\n",
       "   8,\n",
       "   5,\n",
       "   7,\n",
       "   37,\n",
       "   35,\n",
       "   19,\n",
       "   5,\n",
       "   43,\n",
       "   8,\n",
       "   12,\n",
       "   5,\n",
       "   19,\n",
       "   7,\n",
       "   19,\n",
       "   8,\n",
       "   31,\n",
       "   5,\n",
       "   7,\n",
       "   24,\n",
       "   13,\n",
       "   19]],\n",
       " 'label': [[34,\n",
       "   3,\n",
       "   7,\n",
       "   27,\n",
       "   8,\n",
       "   21,\n",
       "   19,\n",
       "   8,\n",
       "   5,\n",
       "   10,\n",
       "   21,\n",
       "   23,\n",
       "   5,\n",
       "   14,\n",
       "   8,\n",
       "   21,\n",
       "   19,\n",
       "   8,\n",
       "   16,\n",
       "   22,\n",
       "   31,\n",
       "   38,\n",
       "   8,\n",
       "   16,\n",
       "   19,\n",
       "   24,\n",
       "   5,\n",
       "   10,\n",
       "   19,\n",
       "   8,\n",
       "   3,\n",
       "   5,\n",
       "   31,\n",
       "   9,\n",
       "   8,\n",
       "   7,\n",
       "   7,\n",
       "   34,\n",
       "   13,\n",
       "   5,\n",
       "   3,\n",
       "   8,\n",
       "   5,\n",
       "   7,\n",
       "   19,\n",
       "   16,\n",
       "   10,\n",
       "   19,\n",
       "   18,\n",
       "   8,\n",
       "   21,\n",
       "   19,\n",
       "   8,\n",
       "   9,\n",
       "   24,\n",
       "   10,\n",
       "   30],\n",
       "  [1]],\n",
       " 'mask': [1, 70]}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_smart.processor.data_raw[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_smart.model.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7547897100448608"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(pred[0]['pred'][1])\n",
    "print(pred[0]['meta'][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
